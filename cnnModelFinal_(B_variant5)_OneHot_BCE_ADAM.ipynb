{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Add, Conv1D, Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.losses import binary_crossentropy, kullback_leibler_divergence\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from custom_metric import rounded_all_or_nothing_acc as RAON_accuracy\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, multilabelled_labels_to_ys, multilabelled_ys_to_labels, onehot_superclass_labels_to_ys, onehot_superclass_ys_to_labels, MULTI_LABEL, ONE_HOT, TIME_SEQUENCE, LINEAR_SPECTROGRAM, LOG_SPECTROGRAM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from keras.callbacks import TensorBoard\n",
    "from time_callback import Time_Callback\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3757258756051357458, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17401617736362168734\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 1},\n",
    "                        log_device_placement = True,\n",
    "                        gpu_options=gpu_options\n",
    "                       )\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "#Loss flags\n",
    "BINARY_CROSSENTROPY = 0\n",
    "KL_DIVERGENCE = 1\n",
    "LOSSES = {BINARY_CROSSENTROPY: binary_crossentropy, KL_DIVERGENCE: kullback_leibler_divergence}\n",
    "\n",
    "# Model config\n",
    "problem_type = ONE_HOT\n",
    "input_type = TIME_SEQUENCE\n",
    "loss = BINARY_CROSSENTROPY\n",
    "optimizer = \"adam\"\n",
    "\n",
    "# Name of model save and log\n",
    "problem_type_str = \"OneHot\" if problem_type == ONE_HOT else \"MultiHot\"\n",
    "input_type_str = {TIME_SEQUENCE: \"(1D)\", LINEAR_SPECTROGRAM: \"(linear 2D)\", LOG_SPECTROGRAM: \"(log 2D)\"}[input_type]  \n",
    "loss_str = \"BCE\" if loss==BINARY_CROSSENTROPY else \"KLD\"\n",
    "optimizer_str = optimizer.upper()\n",
    "date_time_str = str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "model_name = \"FinalModel-v5-{}-{}-{}-{}_{}\".format(input_type_str, problem_type_str, loss_str, optimizer_str, date_time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "batch_size = 50\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True, problem_type=problem_type, input_type=input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen validation\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen test\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n"
     ]
    }
   ],
   "source": [
    "batch_x_shape = None\n",
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_x_shape = batch_0[0].shape\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "### Model structure\n",
    "\n",
    "4 1D casual conv convolution layers which reduce the size of the sample space while increasing the size of the convolution/feature space.\n",
    "\n",
    "Creates feature space of 32, while downscaling the sample space to 500. Compared to 12000, total tensor sizes: 12000 -> 16000 (increase in data).\n",
    "\n",
    "After convoluton layers, LeakyReLU was used for activation because ReLU has been shown to perform well and LeakyReLU takes negatives into account slightly which appear in the data. For this reason He-normal was used to initialise the convolution kernals as this performs well with ReLU.\n",
    "\n",
    "Then 3 lots of \"DilatedDropoutSkipModule\" which retain the size of the sample space while extracting more features. With skip connections preserving earlier features. Finished with dropout layers to aid in generalisation during training.\n",
    "\n",
    "Flattened and passed to a fully-connected (dense layer) which reshapes the network into the output shape.\n",
    "\n",
    "Softmax activation layer for one-hot classification.\n",
    "\n",
    "Techniques from lit review:\n",
    "- LeNet: Convolutions with `stride > 1` to downscale sample space.\n",
    "- Using 1x1 convolution layers to downscale feature space, instead of pooling layers. \n",
    "- Leaky ReLU & He kernal inits.\n",
    "- Dropout for generalisation during training.\n",
    "- ResNet for skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 12000, 1)\n",
      "conv1d_1 (None, 6000, 8)\n",
      "leaky_re_lu_1 (None, 6000, 8)\n",
      "conv1d_2 (None, 3000, 16)\n",
      "leaky_re_lu_2 (None, 3000, 16)\n",
      "conv1d_3 (None, 1500, 32)\n",
      "leaky_re_lu_3 (None, 1500, 32)\n",
      "dropout_1 (None, 1500, 32)\n",
      "conv1d_4 (None, 1500, 1)\n",
      "conv1d_5 (None, 1500, 16)\n",
      "conv1d_6 (None, 1500, 32)\n",
      "leaky_re_lu_4 (None, 1500, 32)\n",
      "add_1 (None, 1500, 32)\n",
      "dropout_2 (None, 1500, 32)\n",
      "conv1d_7 (None, 1500, 1)\n",
      "conv1d_8 (None, 1500, 16)\n",
      "conv1d_9 (None, 1500, 32)\n",
      "leaky_re_lu_5 (None, 1500, 32)\n",
      "add_2 (None, 1500, 32)\n",
      "dropout_3 (None, 1500, 32)\n",
      "conv1d_10 (None, 1500, 1)\n",
      "conv1d_11 (None, 1500, 16)\n",
      "conv1d_12 (None, 1500, 32)\n",
      "leaky_re_lu_6 (None, 1500, 32)\n",
      "add_3 (None, 1500, 32)\n",
      "dropout_4 (None, 1500, 32)\n",
      "conv1d_13 (None, 1500, 1)\n",
      "conv1d_14 (None, 1500, 16)\n",
      "conv1d_15 (None, 1500, 32)\n",
      "leaky_re_lu_7 (None, 1500, 32)\n",
      "add_4 (None, 1500, 32)\n",
      "dropout_5 (None, 1500, 32)\n",
      "conv1d_16 (None, 1500, 1)\n",
      "conv1d_17 (None, 1500, 16)\n",
      "conv1d_18 (None, 1500, 32)\n",
      "leaky_re_lu_8 (None, 1500, 32)\n",
      "add_5 (None, 1500, 32)\n",
      "dropout_6 (None, 1500, 32)\n",
      "flatten_1 (None, 48000)\n",
      "dense_1 (None, 87)\n",
      "activation_1 (None, 87)\n"
     ]
    }
   ],
   "source": [
    "# Variables.\n",
    "NMODULES = 5\n",
    "DROP_RATES = np.around(np.arange(0.12, 0.21, 0.02), 2).tolist() # 6 repeatitions with 0.025 difference in drop rate\n",
    "final_activation = {ONE_HOT: \"softmax\", MULTI_LABEL: \"sigmoid\"}[problem_type]\n",
    "leaky_gradient = 0.25\n",
    "\n",
    "# Reusable dilated convolution / inception module / dropout layer\n",
    "def DilatedDropoutSkipModule(og_model, drop_rate):\n",
    "    model = Conv1D(filters=1, kernel_size=1, padding=\"valid\", dilation_rate=1, kernel_initializer='he_normal')(og_model)\n",
    "    model = Conv1D(filters=16, kernel_size=3, padding=\"causal\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = Conv1D(filters=32, kernel_size=3, padding=\"causal\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = LeakyReLU(leaky_gradient)(model)\n",
    "    model = Add()([og_model, model])\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "# Structure\n",
    "data = Input(shape=(12000, 1))\n",
    "cnn = Conv1D(filters=8, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='he_normal')(data)\n",
    "cnn = LeakyReLU(leaky_gradient)(cnn)\n",
    "cnn = Conv1D(filters=16, kernel_size=3, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='he_normal')(cnn)\n",
    "cnn = LeakyReLU(leaky_gradient)(cnn)\n",
    "cnn = Conv1D(filters=32, kernel_size=3, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='he_normal')(cnn)\n",
    "cnn = LeakyReLU(leaky_gradient)(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for i in range(NMODULES):\n",
    "    cnn = DilatedDropoutSkipModule(cnn, DROP_RATES[i])\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(batch_y_shape[1], kernel_initializer='he_normal')(cnn)\n",
    "cnn = Activation(final_activation)(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# The label specific metric, dependant on the problem type from custom settings.\n",
    "problem_metric = {ONE_HOT: categorical_accuracy, MULTI_LABEL: binary_accuracy}[problem_type]\n",
    "    \n",
    "# Compile with custom settings defined earlier\n",
    "model.compile(optimizer=optimizer, loss=LOSSES[loss], metrics=[problem_metric, RAON_accuracy, LOSSES[(loss + 1)%2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1586/1587 [============================>.] - ETA: 0s - loss: 0.0717 - categorical_accuracy: 0.0551 - rounded_all_or_nothing_acc: 0.0061 - kullback_leibler_divergence: 5.0774"
     ]
    }
   ],
   "source": [
    "# Training logs\n",
    "log_dir = \"logs/{}\".format(model_name)\n",
    "# Tensorboard log\n",
    "tb_log = TensorBoard(log_dir=log_dir)\n",
    "# Custom time log\n",
    "time_log = Time_Callback(log_dir=log_dir)\n",
    "\n",
    "# Train model\n",
    "epochs = 10\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int(N[\"training\"]*dataset_perc) // batch_size,\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int(N[\"validation\"]*dataset_perc) // batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_log, time_log],\n",
    "                use_multiprocessing = use_multiprocessing,\n",
    "                workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model\n",
    "\n",
    "After 10 epochs of training and validation with the training and validation data sets, a seperate evaluation run calculated the final accuracy of the model using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int(N[\"test\"]*dataset_perc) // batch_size,\n",
    "    use_multiprocessing = use_multiprocessing,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of predictions using test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues = [], []\n",
    "for i in range(10):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, batch_x_shape[1], batch_x_shape[2])), batch_y_shape[1])\n",
    "    print(\"Sample\", i)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, onehot_superclass_ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], onehot_superclass_ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    preds.append(pred_y)\n",
    "    trues.append(y)\n",
    "\n",
    "preds_tensor = K.variable(np.array(preds))\n",
    "trues_tensor = K.variable(np.array(trues))\n",
    "problem_acc = K.eval(K.mean(problem_metric(trues_tensor, preds_tensor)))\n",
    "print(\"{} accuracy (avg): {}\".format(\"Categorical\" if problem_type is ONE_HOT else \"Binary\", problem_acc))\n",
    "print(\"RAON accuracy: \", K.eval(RAON_accuracy(trues_tensor, preds_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights to `/models` directory\n",
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
