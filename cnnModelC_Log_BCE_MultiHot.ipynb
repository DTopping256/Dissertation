{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model C (log spectrogram input)\n",
    "## With: multi-hot labelling and binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Add, Conv2D, Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from custom_metric import rounded_all_or_nothing_acc as RAON_accuracy\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, multilabelled_labels_to_ys, multilabelled_ys_to_labels, onehot_superclass_labels_to_ys, onehot_superclass_ys_to_labels, MULTI_LABEL, ONE_HOT, TIME_SEQUENCE, LOG_SPECTROGRAM, LINEAR_SPECTROGRAM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from keras.callbacks import TensorBoard\n",
    "from time_callback import Time_Callback\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11284613977366837717, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 7238689346923368571\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This makes sure that Tensorflow has access to my GPU, as well as my CPU.\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 1},\n",
    "                        log_device_placement = True,\n",
    "                        gpu_options=gpu_options\n",
    "                       )\n",
    "\n",
    "#Loss flags\n",
    "BINARY_CROSSENTROPY = 0\n",
    "KL_DIVERGENCE = 1\n",
    "LOSSES = {BINARY_CROSSENTROPY: binary_crossentropy, KL_DIVERGENCE: kullback_leibler_divergence}\n",
    "\n",
    "# Model config\n",
    "problem_type = MULTI_LABEL\n",
    "input_type = LOG_SPECTROGRAM\n",
    "loss = BINARY_CROSSENTROPY\n",
    "optimizer = \"adam\"\n",
    "\n",
    "# Name of model save and log\n",
    "problem_type_str = \"OneHot\" if problem_type == ONE_HOT else \"MultiHot\"\n",
    "input_type_str = {TIME_SEQUENCE: \"(1D)\", LINEAR_SPECTROGRAM: \"(linear 2D)\", LOG_SPECTROGRAM: \"(log 2D)\"}[input_type]  \n",
    "loss_str = \"BCE\" if loss == BINARY_CROSSENTROPY else \"KLD\"\n",
    "optimizer_str = optimizer.upper()\n",
    "date_time_str = str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "model_name = \"ModelC-{}-{}-{}-{}_{}\".format(input_type_str, problem_type_str, loss_str, optimizer_str, date_time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "batch_size = 50\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True, problem_type=problem_type, input_type=input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 10)\n",
      "Gen validation\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 10)\n",
      "Gen test\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_x_shape = None\n",
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_x_shape = batch_0[0].shape\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "### Model structure\n",
    "\n",
    "2 \"2DConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution/feature space. Causal convolutions don't exist for 2D (yet) so fully connected 2D convolution will suffice.\n",
    "\n",
    "Creates feature space of 32, while downscaling the sample space to 33x14 (462). Compared to 129x53 (6837), total tensor sizes: 341850 -> 739200 (increase in data).\n",
    "\n",
    "After convoluton layers, LeakyReLU was used for activation because ReLU has been shown to perform well and LeakyReLU takes negatives into account slightly which appear in the data. For this reason He-normal was used to initialise the convolution kernals as this performs well with ReLU.\n",
    "\n",
    "Then 3 lots of \"DilatedDropoutSkipModule\" which retain the size of the sample space while extracting more features. With skip connections preserving earlier features. Finished with dropout layers to aid in generalisation during training.\n",
    "\n",
    "Flattened and passed to a fully-connected (dense layer) which reshapes the network into the output shape.\n",
    "\n",
    "Softmax activation layer for one-hot classification.\n",
    "\n",
    "Techniques from lit review:\n",
    "- LeNet: Convolutions with `stride > 1` to downscale sample space.\n",
    "- Using 1x1 convolution layers to downscale feature space, instead of pooling layers. \n",
    "- Leaky ReLU & He kernal inits.\n",
    "- Dropout for generalisation during training.\n",
    "- ResNet for skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 129, 53, 1)\n",
      "conv2d_1 (None, 65, 27, 16)\n",
      "leaky_re_lu_1 (None, 65, 27, 16)\n",
      "conv2d_2 (None, 33, 14, 32)\n",
      "leaky_re_lu_2 (None, 33, 14, 32)\n",
      "dropout_1 (None, 33, 14, 32)\n",
      "conv2d_3 (None, 33, 14, 1)\n",
      "conv2d_4 (None, 33, 14, 8)\n",
      "conv2d_5 (None, 33, 14, 32)\n",
      "leaky_re_lu_3 (None, 33, 14, 32)\n",
      "add_1 (None, 33, 14, 32)\n",
      "dropout_2 (None, 33, 14, 32)\n",
      "conv2d_6 (None, 33, 14, 1)\n",
      "conv2d_7 (None, 33, 14, 8)\n",
      "conv2d_8 (None, 33, 14, 32)\n",
      "leaky_re_lu_4 (None, 33, 14, 32)\n",
      "add_2 (None, 33, 14, 32)\n",
      "dropout_3 (None, 33, 14, 32)\n",
      "conv2d_9 (None, 33, 14, 1)\n",
      "conv2d_10 (None, 33, 14, 8)\n",
      "conv2d_11 (None, 33, 14, 32)\n",
      "leaky_re_lu_5 (None, 33, 14, 32)\n",
      "add_3 (None, 33, 14, 32)\n",
      "dropout_4 (None, 33, 14, 32)\n",
      "flatten_1 (None, 14784)\n",
      "dense_1 (None, 10)\n",
      "activation_1 (None, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reusable series of layers\n",
    "def DilatedDropoutSkipModule(og_model, drop_rate):\n",
    "    model = Conv2D(filters=1, kernel_size=1, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(og_model)\n",
    "    model = Conv2D(filters=8, kernel_size=3, padding=\"same\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = Conv2D(filters=32, kernel_size=3, padding=\"same\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = LeakyReLU(0.3)(model)\n",
    "    model = Add()([og_model, model])\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "# The dropout rates and amount of modules.\n",
    "NMODULES = 3\n",
    "DROP_RATES = [0.15, 0.2, 0.25]\n",
    "\n",
    "final_activation = {ONE_HOT: \"softmax\", MULTI_LABEL: \"sigmoid\"}[problem_type]\n",
    "\n",
    "# Structure\n",
    "data = Input(shape=(129, 53, 1))\n",
    "cnn = Conv2D(filters=16, kernel_size=5, strides=2, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(data)\n",
    "cnn = LeakyReLU(0.3)(cnn)\n",
    "cnn = Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(cnn)\n",
    "cnn = LeakyReLU(0.3)(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for i in range(NMODULES):\n",
    "    cnn = DilatedDropoutSkipModule(cnn, DROP_RATES[i])\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(batch_y_shape[1], kernel_initializer='he_normal')(cnn)\n",
    "cnn = Activation(final_activation)(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# The label specific metric, dependant on the problem type from custom settings.\n",
    "problem_metric = {ONE_HOT: categorical_accuracy, MULTI_LABEL: binary_accuracy}[problem_type]\n",
    "    \n",
    "# Compile with custom settings defined earlier\n",
    "model.compile(optimizer=optimizer, loss=LOSSES[loss], metrics=[problem_metric, RAON_accuracy, LOSSES[(loss + 1)%2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1587/1587 [==============================] - 1445s 910ms/step - loss: 0.5061 - acc: 0.8016 - rounded_all_or_nothing_acc: 0.0267 - kullback_leibler_divergence: 3.1963 - val_loss: 0.4323 - val_acc: 0.8135 - val_rounded_all_or_nothing_acc: 0.0393 - val_kullback_leibler_divergence: 3.3157\n",
      "Epoch 2/10\n",
      "1587/1587 [==============================] - 1334s 840ms/step - loss: 0.3375 - acc: 0.8605 - rounded_all_or_nothing_acc: 0.1895 - kullback_leibler_divergence: 2.0818 - val_loss: 0.2840 - val_acc: 0.8840 - val_rounded_all_or_nothing_acc: 0.2602 - val_kullback_leibler_divergence: 1.8650\n",
      "Epoch 3/10\n",
      "1587/1587 [==============================] - 1189s 749ms/step - loss: 0.2852 - acc: 0.8851 - rounded_all_or_nothing_acc: 0.2827 - kullback_leibler_divergence: 1.7511 - val_loss: 0.2515 - val_acc: 0.9016 - val_rounded_all_or_nothing_acc: 0.3370 - val_kullback_leibler_divergence: 1.5491\n",
      "Epoch 4/10\n",
      "1587/1587 [==============================] - 2542s 2s/step - loss: 0.2646 - acc: 0.8943 - rounded_all_or_nothing_acc: 0.3198 - kullback_leibler_divergence: 1.6202 - val_loss: 0.2371 - val_acc: 0.9071 - val_rounded_all_or_nothing_acc: 0.3678 - val_kullback_leibler_divergence: 1.4648\n",
      "Epoch 5/10\n",
      "1587/1587 [==============================] - 2578s 2s/step - loss: 0.2508 - acc: 0.9006 - rounded_all_or_nothing_acc: 0.3475 - kullback_leibler_divergence: 1.5346 - val_loss: 0.2251 - val_acc: 0.9120 - val_rounded_all_or_nothing_acc: 0.3853 - val_kullback_leibler_divergence: 1.4246\n",
      "Epoch 6/10\n",
      "1587/1587 [==============================] - 2576s 2s/step - loss: 0.2412 - acc: 0.9042 - rounded_all_or_nothing_acc: 0.3620 - kullback_leibler_divergence: 1.4747 - val_loss: 0.2162 - val_acc: 0.9162 - val_rounded_all_or_nothing_acc: 0.4079 - val_kullback_leibler_divergence: 1.3561\n",
      "Epoch 7/10\n",
      "1587/1587 [==============================] - 2588s 2s/step - loss: 0.2331 - acc: 0.9079 - rounded_all_or_nothing_acc: 0.3822 - kullback_leibler_divergence: 1.4240 - val_loss: 0.2100 - val_acc: 0.9187 - val_rounded_all_or_nothing_acc: 0.4292 - val_kullback_leibler_divergence: 1.3246\n",
      "Epoch 8/10\n",
      "1587/1587 [==============================] - 1579s 995ms/step - loss: 0.2280 - acc: 0.9099 - rounded_all_or_nothing_acc: 0.3920 - kullback_leibler_divergence: 1.3915 - val_loss: 0.2051 - val_acc: 0.9207 - val_rounded_all_or_nothing_acc: 0.4403 - val_kullback_leibler_divergence: 1.2299\n",
      "Epoch 9/10\n",
      "1587/1587 [==============================] - 1370s 863ms/step - loss: 0.2228 - acc: 0.9119 - rounded_all_or_nothing_acc: 0.4005 - kullback_leibler_divergence: 1.3579 - val_loss: 0.2000 - val_acc: 0.9222 - val_rounded_all_or_nothing_acc: 0.4364 - val_kullback_leibler_divergence: 1.2952\n",
      "Epoch 10/10\n",
      "1587/1587 [==============================] - 1407s 886ms/step - loss: 0.2193 - acc: 0.9136 - rounded_all_or_nothing_acc: 0.4106 - kullback_leibler_divergence: 1.3372 - val_loss: 0.1951 - val_acc: 0.9250 - val_rounded_all_or_nothing_acc: 0.4561 - val_kullback_leibler_divergence: 1.2433\n"
     ]
    }
   ],
   "source": [
    "# Training logs\n",
    "log_dir = \"logs/{}\".format(model_name)\n",
    "# Tensorboard log\n",
    "tb_log = TensorBoard(log_dir=log_dir)\n",
    "# Custom time log\n",
    "time_log = Time_Callback(log_dir=log_dir)\n",
    "\n",
    "# Train model\n",
    "epochs = 10\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int(N[\"training\"]*dataset_perc) // batch_size,\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int(N[\"validation\"]*dataset_perc) // batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_log, time_log],\n",
    "                use_multiprocessing = use_multiprocessing,\n",
    "                workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model\n",
    "\n",
    "After 10 epochs of training and validation with the training and validation data sets, a seperate evaluation run calculated the final accuracy of the model using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19478438286618752,\n",
       " 0.9251249974424188,\n",
       " 0.4533333223821087,\n",
       " 1.2430265752147094]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int(N[\"test\"]*dataset_perc) // batch_size,\n",
    "    use_multiprocessing = use_multiprocessing,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights to `/models` directory\n",
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of predictions using test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Actual:\n",
      "\t[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "Prediction:\n",
      "\t[0.133, 0.0, 0.0, 1.0, 0.008, 0.001, 0.001, 0.017, 0.01, 0.013],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "\n",
      "Sample 1\n",
      "Actual:\n",
      "\t[1. 0. 1. 0. 0. 0. 0. 0. 1. 0.],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.298, 0.011, 0.511, 0.002, 0.08, 0.061, 0.002, 0.289, 0.402, 0.004],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 2\n",
      "Actual:\n",
      "\t[0. 0. 0. 1. 1. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "Prediction:\n",
      "\t[0.219, 0.05, 0.02, 0.918, 0.388, 0.353, 0.03, 0.016, 0.216, 0.001],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "\n",
      "Sample 3\n",
      "Actual:\n",
      "\t[1. 0. 0. 0. 0. 1. 0. 1. 0. 0.],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.837, 0.127, 0.019, 0.069, 0.187, 0.321, 0.075, 0.824, 0.036, 0.009],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 4\n",
      "Actual:\n",
      "\t[1. 1. 0. 0. 0. 0. 0. 0. 0. 1.],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.998, 0.343, 0.003, 0.003, 0.015, 0.01, 0.405, 0.007, 0.005, 0.244],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 5\n",
      "Actual:\n",
      "\t[1. 0. 1. 0. 0. 0. 0. 1. 0. 0.],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.724, 0.357, 0.058, 0.068, 0.042, 0.358, 0.202, 0.059, 0.066, 0.076],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 6\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 1. 1. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.255, 0.031, 0.011, 0.007, 0.498, 0.55, 0.646, 0.006, 0.142, 0.059],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 7\n",
      "Actual:\n",
      "\t[0. 0. 1. 0. 0. 0. 0. 1. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.015, 0.003, 0.998, 0.041, 0.01, 0.016, 0.001, 0.93, 0.203, 0.001],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 8\n",
      "Actual:\n",
      "\t[1. 1. 0. 0. 0. 1. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.649, 0.991, 0.017, 0.021, 0.331, 0.164, 0.207, 0.011, 0.034, 0.072],\n",
      "\t{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 9\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['high_tom'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.02, 0.021, 0.03, 0.001, 0.952, 0.006, 0.002, 0.012, 0.222, 0.976],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['high_tom'], 'tech_label': ['normal']}\n",
      "\n",
      "Categorical_accuracy (avg) 0.6\n",
      "RAON accuracy:  0.4\n"
     ]
    }
   ],
   "source": [
    "preds, trues = [], []\n",
    "for i in range(10):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, 129, 53, 1)), batch_y_shape[1])\n",
    "    print(\"Sample\", i)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, onehot_superclass_ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], onehot_superclass_ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    preds.append(pred_y)\n",
    "    trues.append(y)\n",
    "\n",
    "preds_tensor = K.variable(np.array(preds))\n",
    "trues_tensor = K.variable(np.array(trues))\n",
    "problem_acc = K.eval(K.mean(problem_metric(trues_tensor, preds_tensor)))\n",
    "print(\"{} accuracy (avg): {}\".format(\"Categorical\" if problem_type is ONE_HOT else \"Binary\", problem_acc))\n",
    "print(\"RAON accuracy: \", K.eval(RAON_accuracy(trues_tensor, preds_tensor)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
