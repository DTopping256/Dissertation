@article{Yokoyama2016,
author = {Yokoyama, Masao and Awahara, Yoshiki and Yagawa, Genki},
doi = {10.1121/1.4971030},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yokoyama, Awahara, Yagawa - 2016 - Relation between violin timbre and harmony overtone.pdf:pdf},
%issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {4},
pages = {3427--3427},
publisher = {Acoustical Society of America (ASA)},
title = {{Relation between violin timbre and harmony overtone}},
volume = {140},
year = {2016}
}
@unpublished{Springenberg2014,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
doi = {10.1163/_q3_SIM_00374},
eprint = {1412.6806},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/pooling{\_}vs{\_}sparce{\_}convolution.pdf:pdf},
%isbn = {9781600066634},
%issn = {02548704},
pages = {1--14},
pmid = {974},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2014}
}
@online{Raschka,
annote = {Accessed on 17-10-2018},
author = {Raschka, Sebastian},
title = {{Single-Layer Neural Networks and Gradient Descent}},
url = {https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html},
%urldate = {2019-01-12},
year = {2015}
}
@book{MacKay2013,
abstract = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first or second year undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single eld, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
author = {MacKay, David J. C.},
booktitle = {Choice Reviews Online},
doi = {10.5860/choice.41-5949},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/information-theory.pdf:pdf},
%isbn = {978-0521642989},
%issn = {0009-4978},
number = {10},
title = {{Information theory, inference, and learning algorithms}},
url = {https://www.inference.org.uk/itprnn/book.pdf},
volume = {41},
year = {2013}
}
@techreport{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Hinton, Geoffrey E and Sutskever, Ilya},
booktitle = {Neural Information Processing Systems},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/ImageNet.pdf:pdf},
institution = {Univeristy of Toronto},
%isbn = {9781627480031},
%issn = {10495258},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@book{Russell;Norvig1995,
abstract = {The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Neurocomputing},
doi = {10.1016/0925-2312(95)90020-9},
%isbn = {9780131038059},
%issn = {09252312},
number = {2},
pages = {215--218},
pmid = {20949757},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://portal.acm.org/citation.cfm?id=773294},
volume = {9},
year = {1995}
}
@techreport{Paulus2010,
abstract = {This thesis proposes signal processing methods for the analysis of musical audio on two time scales: drum transcription on a finer time scale and music structure analysis on the time scale of entire pieces. The former refers to the process of locating drum sounds in the input and recognising the instruments that were used to produce the sounds. The latter refers to the temporal segmentation of a musical piece into parts, such as chorus and verse. For drum transcription, both low-level acoustic recognition and high-level musicological modelling methods are presented. A baseline acoustic recognition method with a large number of features using Gaussian mixture models for the recognition of drum combinations is presented. Since drums occur in structured patterns, modelling of the sequential dependencies with N-grams is proposed. In addition to the conventional N-grams, periodic N-grams are proposed to model the dependencies between events that occur one pattern length apart. The evaluations show that incorporating musicological modelling improves the performance considerably. As some drums are more probable to occur at certain points in a pattern, this dependency is utilised for producing transcriptions of signals produced with arbitrary sounds, such as beatboxing. A supervised source separation method using non-negative matrix factorisation is proposed for transcribing mixtures of drum sounds. Despite the simple signal model, a high performance is obtained for signals without other instruments. Most of the drum transcription methods operate only on single-channel inputs, but multichannel signals are available in recording studios. A multichannel extension of the source separation method is proposed, and an increase in performance is observed in evaluations. Many of the drum transcription methods rely on detecting sound onsets for the segmentation of the signal. Detection errors will then decrease the overall performance of the system. To overcome this problem, a method utilising a network of connected hidden Markov models is proposed to perform the event segmentation and recognition jointly. The system is shown to be able to perform the transcription even from polyphonic music. The second main topic of this thesis is music structure analysis. Two methods are proposed for this purpose. The first relies on defining a cost function for a description of the repeated parts. The second method defines a fitness function for descriptions covering the entire piece. The abstract cost (and fitness) functions are formulated in terms that can be determined from the input signal algorithmically, and optimisation problems are formulated. In both cases, an algorithm is proposed for solving the optimisation problems. The first method is evaluated on a small data set, and the relevance of the cost function terms is shown. The latter method is evaluated on three large data sets with a total of 831 (557+174+100) songs. This is to date the largest evaluation of a structure analysis method. The evaluations show that the proposed method outperforms a reference system on two of the data sets. Music structure analysis methods rarely provide musically meaningful names for the parts in the result. A method is proposed to label the parts in descriptions based on a statistical model of the sequential dependencies between musical parts. The method is shown to label the main parts relatively reliably without any additional information. The labelling model is further integrated into the fitness function based structure analysis method.},
author = {Paulus, Jouni},
booktitle = {{\ldots}teknillinen yliopisto. Julkaisu-Tampere University of {\ldots}},
doi = {DOI: 10.1007/s11214-007-9186-2},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/paulus{\_}phd.pdf:pdf},
%isbn = {9789521523052},
title = {{Signal processing methods for drum transcription and music structure analysis}},
url = {http://dspace.cc.tut.fi/dpub/handle/123456789/6445},
year = {2010}
}
@misc{MNIST,
abstract = {Current analytical models of grasping and manipulation with robotic hands contain simplifications and assumptions that limit their application to manufacturing environments. To evaluate these models, a study was undertaken of the grasps used by machinists in a small batch manufacturing operation. Based on the study, a taxonomy of grasps was constructed. An expert system was also developed to clarify the issues involved in human grasp choice. Comparisons of the grasp taxonomy, the expert system, and grasp-quality measures derived from the analytic models reveal that the analytic measures are useful for describing grasps in manufacturing tasks despite the limitations in the models. In addition, the grasp taxonomy provides insights for the design of versatile robotic hands for manufacturing},
annote = {Accessed on 22/10/2018},
author = {LeCunn, Yann},
booktitle = {html(MNIST dataset)},
doi = {10.1109/70.34763},
%isbn = {7842500200015},
%issn = {1042-296X},
pages = {1--8},
pmid = {1000253529},
title = {{THE MNIST DATABASE of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {2010}
}
@techreport{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
booktitle = {Journal of Artificial Intelligence Research},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:pdf},
pages = {321--357},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
volume = {16},
year = {2002}
}
@techreport{Hubel1959,
abstract = {Recordings were made in lightly anaesthesized cats whose retinas were stimulated, singly or together, with light spots of various sizes and shapes. Receptive fields, defined as restricted areas where illumination influenced the firing of a single cortical unit, usually contained mutually antagonistic excitatory and inhibitory regions. Thus a stimulus covering a whole field was relatively ineffective in driving most units. Effective driving of a unit required a stimulus specific in form, size, position, and orientation; based on the arrangement of excitatory and inhibitory areas. About 20{\%} of the cortical units studied could be activated from either eye; these were driven from roughly homologous regions of the retinas and summation and antagonism could be shown. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Hubel, D. H. and Wiesel, T. N.},
booktitle = {The Journal of Physiology},
doi = {10.1113/jphysiol.1959.sp006308},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/Hubel{\_}et{\_}al-1959-The{\_}Journal{\_}of{\_}Physiology.pdf:pdf},
%isbn = {0022-3751 (Print)},
%issn = {14697793},
number = {3},
pages = {574--591},
pmid = {14403679},
title = {{Receptive fields of single neurones in the cat's striate cortex}},
url = {http://doi.wiley.com/10.1113/jphysiol.1959.sp006308},
volume = {148},
year = {1959}
}
@misc{lunaverus,
author = {Lunaverus},
keywords = {lunaveras},
mendeley-tags = {lunaveras},
title = {{Music Transcription with Convolutional Neural Networks}},
url = {https://www.lunaverus.com/cnn},
%urldate = {2019-04-02},
year = {2019}
}
@article{Nwankpa2018,
abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
archivePrefix = {arXiv},
arxivId = {1811.03378},
author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
eprint = {1811.03378},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nwankpa et al. - 2018 - Activation Functions Comparison of trends in Practice and Research for Deep Learning.pdf:pdf},
month = {nov},
title = {{Activation Functions: Comparison of trends in Practice and Research for Deep Learning}},
url = {http://arxiv.org/abs/1811.03378},
year = {2018}
}
@article{McCullock_Pitts,
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCullock, Warren and Pitts, Walter},
doi = {10.1007/BF02459570},
%isbn = {0092-8240},
journal = {Bulletin of mathematical biology},
number = {1-2},
pages = {99--115.},
publisher = {Bulletin of Mathematical Biophysics},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://www.researchgate.net/publication/20969919_A_logical_calculus_of_the_ideas_immanent_in_nervous_activity_1943},
volume = {52},
year = {1943}
}
@book{Rojas,
author = {Rojas, Raul},
doi = {10.1109/78.127967},
%isbn = {9783540605058},
%issn = {1053587X},
pages = {152 -- 184},
publisher = {Springer-Verlag},
title = {{Neural Networks - A Systematic Introduction - Backpropagation}},
url = {https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf},
year = {1996}
}
@misc{Nielsen,
annote = {Accessed on 22-10-2018},
author = {Nielsen, Michael A},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/chap6.html},
%urldate = {2019-03-02},
year = {2018}
}
@article{Asadi2007,
abstract = {Learning capabilities of computer systems still lag far behind biological systems. One of the reasons can be seen in the inefficient re-use of control knowledge acquired over the lifetime of the artificial learning system. To address this deficiency, this paper presents a learning architecture which transfers control knowledge in the form of behavioral skills and corresponding representation concepts from one task to subsequent learning tasks. The presented system uses this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy. Experimental results show that the presented method can significantly outperform learning on a flat state space representation and the MAXQ method for hierarchical reinforcement learning.},
author = {Asadi, Mehran and Huber, Manfred},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/transfer{\_}learning.pdf:pdf},
%issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Markov-decision processes,learning,reactive control},
pages = {2054--2059},
title = {{Effective control knowledge transfer through learning skill and representation hierarchies}},
year = {2007}
}
@techreport{Hinton2014,
author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/JMLRdropout.pdf:pdf},
institution = {University of Toronto},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf},
year = {2014}
}
@book{Cawsey,
abstract = {A concise, practical introduction to artificial intelligence, this title starts with the fundamentals of knowledge representation, inference, expert systems, natural language processing, machine learning, neural networks, agents, robots, and much more. Examples and algorithms are presented throughout, and the book includes a complete glossary},
author = {Cawsey, Alison},
booktitle = {Essence of computing series},
doi = {10.1037/h0072647},
%isbn = {0135717795 (pbk. alk. paper)},
%issn = {0033295X},
keywords = {Artificial intelligence.,Expert systems (Computer science)},
pages = {ix, 190 p.},
pmid = {4015403},
publisher = {Prentice Hall Europe},
title = {{The essence of artificial intelligence}},
url = {http://www.loc.gov/catdir/toc/fy034/97011460.html},
year = {1998}
}
@book{Muller,
abstract = {This textbook provides both profound technological knowledge and a comprehensive treatment of essential topics in music processing and music information retrieval. Including numerous examples, figures, and exercises, this book is suited for students, lecturers, and researchers working in audio engineering, computer science, multimedia, and musicology.},
author = {M{\"{u}}ller, Meinard},
doi = {10.1007/978-3-319-21945-5},
%isbn = {978-3-319-21944-8},
%issn = {1029-8649},
pmid = {12525892},
publisher = {Springer-Verlag},
title = {{Fundamentals of Music Processing}},
url = {http://link.springer.com/10.1007/978-3-319-21945-5},
year = {2015}
}
@techreport{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, V and international Conference, GE Hinton - Proceedings of the 27th and 2010, Undefined},
booktitle = {Cs.Toronto.Edu},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/reluICML.pdf:pdf},
institution = {University of Toronto},
%isbn = {9781605589077},
%issn = {1935-8237},
pages = {6421113},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
url = {https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf},
year = {2010}
}
@misc{SciPy,
abstract = {SciPy (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.},
author = {SciPy},
title = {{SciPy.org}},
url = {https://www.scipy.org/},
%urldate = {2019-04-30},
year = {2019}
}
@misc{Librosa,
abstract = {A python package for music and audio analysis.},
author = {Librosa},
title = {{Librosa library}},
url = {https://github.com/librosa/librosa},
year = {2019}
}
@misc{merriam-webster,
author = {Merriam-Webster},
title = {{Merriam-Webster}},
url = {https://www.merriam-webster.com/dictionary/waveform},
%urldate = {2019-04-30},
year = {2019}
}
@article{HeNorm2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%}). To our knowledge, our result is the first to surpass human-level performance (5.1{\%}, Russakovsky et al.) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
month = {feb},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://arxiv.org/abs/1502.01852},
year = {2015}
}
@misc{Keras,
author = {Keras},
publisher = {Keras},
title = {{Keras metrics source-code}},
url = {https://github.com/keras-team/keras},
year = {2019}
}
@techreport{Maas2013,
abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2{\%} absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maas, Hannun, Ng - 2013 - Rectifier Nonlinearities Improve Neural Network Acoustic Models.pdf:pdf},
title = {{Rectifier Nonlinearities Improve Neural Network Acoustic Models}},
year = {2013}
}
@techreport{Paulus2005,
abstract = {This paper describes a novel method for the automatic transcrip- tion of drum sequences. The method is based on separating the target drum sounds from the input signal using non-negative ma- trix factorisation, and on detecting sound onsets from the separated signals. The separation algorithm factorises the spectrogram of the input signal into a sum of instrument spectrograms, each having a fixed spectrum and a time-varying gain. The spectra are cal- culated from a set of training signals, and the time-varying gains are estimated with an algorithm stemming from non-negative ma- trix factorisation. Onset times of the instruments are detected from the estimated time-varying gains. The system gave better results than two state-of-the-art methods in simulations with acoustic sig- nals containing polyphonic drum sequences, and overall hit rate of 96{\%} was accomplished. Demonstrational signals are available at http://www.cs.tut.fi/˜paulus/demo/.},
author = {Paulus, J and Virtanen, T},
booktitle = {Proc. of 13th European Signal Processing Conference (EUSIPCO2005)},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/eusipco05{\_}paulus.pdf:pdf},
%isbn = {978-160-4238-21-1},
title = {{Drum transcription with non-negative spectrogram factorisation}},
url = {http://www.cs.tut.fi/sgn/arg/paulus/eusipco05_paulus.pdf},
year = {2005}
}
@article{Benetos,
abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
doi = {10.1007/s10844-013-0258-3},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/AMT.pdf:pdf},
%issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Automatic music transcription,Music information retrieval,Music signal analysis},
number = {3},
pages = {407--434},
title = {{Automatic music transcription: Challenges and future directions}},
volume = {41},
year = {2013}
}
@unpublished{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/ResNet.pdf:pdf},
institution = {Microsoft Research},
%isbn = {978-1-4673-8851-1},
%issn = {1664-1078},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Glorot2010,
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/weight initialisation.pdf:pdf},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
volume = {9},
year = {2010}
}
@article{Bello2005,
abstract = {Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for “transient” regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application.},
author = {Bello, Juan Pablo and Daudet, Laurent and Abdallah, Samer and Duxbury, Chris and Davies, Mike and Sandler, Mark B.},
doi = {10.1109/TSA.2005.851998},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/onset{\_}detection.pdf:pdf},
%isbn = {1063-6676},
%issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Attack transcients,Audio,Note segmentation,Novelty detection},
number = {5},
pages = {1035--1046},
pmid = {1000106150},
title = {{A tutorial on onset detection in music signals}},
volume = {13},
year = {2005}
}
@unpublished{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
address = {London},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {1609.03499},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/WaveNet.pdf:pdf},
institution = {Google},
%isbn = {9783901882760},
%issn = {0899-7667},
pages = {1--15},
pmid = {18785855},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@techreport{Rosenblatt,
author = {Rosenblatt, F},
booktitle = {Psychological Review},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenblatt - Unknown - THE PERCEPTRON A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1.pdf:pdf},
institution = {Cornell Aeronautical Laboratory},
number = {6},
pages = {19--27},
title = {{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}},
volume = {65},
year = {1958}
}
@report{gmsw2016,
abstract = {In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.08764v1},
author = {Wong, Sebastien C. and Gatt, Adam and Stamatescu, Victor and McDonnell, Mark D.},
booktitle = {2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
doi = {10.1109/DICTA.2016.7797091},
eprint = {arXiv:1609.08764v1},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/augmentation.pdf:pdf},
%isbn = {9781509028962},
title = {{Understanding Data Augmentation for Classification: When to Warp?}},
url = {https://arxiv.org/abs/1609.08764v2},
year = {2016}
}
@techreport{Lecun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.},
author = {Lecun, Yann and Bottou, Eon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
institution = {IEEE},
keywords = {Convolutional neural networks,document recog-nition,finite state transducers,gradient-based learning,graph transformer networks,machine learning,neural networks,optical character recognition (OCR)},
title = {{Gradient-Based Learning Applied to Document Recognition}},
url = {https://ieeexplore.ieee.org/document/726791},
year = {1998}
}
@techreport{Powers2007,
abstract = {Commonly used evaluation measures including Recall, Precision, F-Factor and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
address = {Adelaide},
author = {Powers, David M W},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Evaluation From Precision, Recall and F-Factor to ROC, Informedness, Markedness {\&}amp Correlation.pdf:pdf},
institution = {Flinders University of South Australia},
keywords = {Bookmaker Informedness,Chi-Squared Significance,Cohen Kappa,Evenness,F-Factor,Log-Likelihood Significance,Markedness,Matthews Correlation,Pearson Correlation,Precision,Rand Accuracy,Recall},
title = {{Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness {\&} Correlation}},
year = {2007}
}
@techreport{Stallard,
abstract = {We have been studying the use of spectral imagery to locate targets in spectrally interfering backgrounds. In making performance estimates for various sensors it has become evident that some crdculations are unreliable because of overflying. Hence, we began a thorough study of the problem of overi{\%}tiug in multivariate classification. In this paper we present some model based results describing the problem. From the model we know the ideal covariance matrix, the ideal discriminant vector, and the ideal classification performance. We then investigate how experimental conditions such as noise, number of bands, and number of samples cause discrepancies from the ideal results. We also suggest ways to discover and alleviate overfitting.},
address = {Albuquerque},
author = {Stallard, Brian R and Taylor, John G},
file = {:C$\backslash$:/Users/danzi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stallard, Taylor - Unknown - Quantifying multivariate classification performance-the problem of overfitting.pdf:pdf},
institution = {Sandia National Laboratories},
keywords = {imaging spectrometry,multivariate classification,overfitting},
title = {{Quantifying multivariate classification performance-the problem of overfitting}},
url = {https://www.researchgate.net/publication/253914692_Quantifying_multivariate_classification_performance_the_problem_of_overfitting},
year = {1999}
}
@unpublished{Ioffe,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer in- puts. Our method draws its strength frommaking normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch- normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters. 1},
author = {Ioffe, Sersey and Szegedy, Christian},
doi = {1502.03167v3},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/batch{\_}normalization.pdf:pdf},
institution = {Google},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {https://arxiv.org/abs/1502.03167v3},
year = {2015}
}
@report{ghlmowyzz2017,
abstract = {Dental intrusion and avulsion, crown fracture and mandibular fractures are important dentofacial complications in patients with epilepsy-related traumas. The objective of the present study was to describe the occurrence of orofacial injuries in patients with epilepsy. One hundred and nine consecutive patients (60 women; mean age 38.81 +/- 14 years), treated for refractory epilepsy (45 with extratemporal epilepsy and 64 with temporal epilepsy) at the outpatient clinic of our University Hospital, were included in the present study. Orofacial injury occurring as a direct result of a seizure was determined by clinical examination and interview. In addition, seizure frequency, use of medication, and the occurrence and type of injury to other parts of the body, were documented. We employed regression analyses to investigate the association between teeth fractures and frequency of seizures. The majority of injuries were crown fractures (42 subjects), followed by mandibular fractures (eight subjects) and tooth avulsion (eight subjects). Sixteen patients had more than two fractured teeth. Patients with mandibular trauma also suffered concomitant injuries (teeth fracture, avulsion and dislocation). The number of fractured teeth was associated with seizure frequency (r(2) = 0.59, p {\textless} 0.001). The data suggest that there is an increased rate of dentoalveolar and maxillofacial injuries in patients with poorly controlled epileptic seizures.},
author = {Maxwell, Andrew and Li, Runzhi and Yang, Bei and Weng, Heng and Ou, Aihua and Hong, Huixiao and Zhou, Zhaoxian and Gong, Ping and Zhang, Chaoyang},
booktitle = {BMC Bioinformatics},
doi = {10.1186/s12859-017-1898-z},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/Multilabel.pdf:pdf},
institution = {BMC Bioinformatics},
%isbn = {1294-9361 (Print)1294-9361 (Linking)},
%issn = {14712105},
keywords = {Deep learning,Deep neural networks,Intelligent health risk prediction,Medical health records,Multi-label classification},
pmid = {21393092},
title = {{Deep learning architectures for multi-label classification of intelligent health risk prediction}},
volume = {18},
year = {2017}
}
@unpublished{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:D$\backslash$:/Documents/University/Year3/Independant Studies/Dissertation/Reference Papers/GoogLeNet.pdf:pdf},
institution = {Computer vision foundation},
%isbn = {9781467369640},
%issn = {10636919},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}