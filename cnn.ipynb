{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Activation, concatenate, Conv1D, Dense, Dropout, Flatten, Input, Lambda\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time as timestamp\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kltls = ['bass_drum-normal','hi_hat-normal',\n",
    "  'hi_hat-open',\n",
    "  'high_tom-normal',\n",
    "  'ride-normal',\n",
    "  'ride-bell',\n",
    "  'crash-normal',\n",
    "  'snare-normal',\n",
    "  'low_tom-normal',\n",
    "  'mid_tom-normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_ys(labels):\n",
    "    ys = np.zeros(len(kltls))\n",
    "    for n in range(len(kltls)):\n",
    "        kl, tl = kltls[n].split(\"-\")\n",
    "        for label_i in range(len(labels[\"hit_label\"])):\n",
    "            if (kl in labels[\"kit_label\"][label_i] and tl in labels[\"tech_label\"][label_i]):\n",
    "                ys[n] = 1\n",
    "    return ys\n",
    "\n",
    "def ys_to_labels(ys, threshold = 0.6):\n",
    "    labels = {\"hit_label\": [], \"kit_label\": [], \"tech_label\": []}\n",
    "    for n in range(len(kltls)):\n",
    "        kl, tl = kltls[n].split(\"-\")\n",
    "        if (ys[n] > threshold):\n",
    "            hl = \"beater\" if kl == \"bass_drum\" else \"stick\"\n",
    "            labels[\"hit_label\"].append(hl)\n",
    "            labels[\"kit_label\"].append(kl)\n",
    "            labels[\"tech_label\"].append(tl)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenerator(Sequence):\n",
    "    def get_ys(self, labels):\n",
    "        return labels_to_ys(labels)\n",
    "    \n",
    "    def __init__(self, filenames, labels, data_type):\n",
    "        self.filenames, self.labels, self.batch_size = filenames, labels, SETTINGS.data[data_type][\"batch_size\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array([np.loadtxt(file_name) for file_name in batch_x]).reshape(100, 12000, 1), np.array(list(map(self.get_ys, batch_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "generators = {\"training\": None, \"test\": None}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In shape: (100, 12000, 1) \n",
      "Out shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_0 = generators[\"training\"].__getitem__(0)\n",
    "print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "def InceptionModule(model):\n",
    "    # Skip connection (uses input in concat)\n",
    "    skip = Lambda(lambda x: x)(model)\n",
    "    # Size 1 kernel conv of input (with tanh activation)\n",
    "    conv_1_tower = Conv1D(filters=128, kernel_size=1, strides=1, padding=\"valid\", activation=\"tanh\")(model)\n",
    "    # Size 1 -> size 3 kernel conv of input (with tanh activation)\n",
    "    conv_3_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_3_tower = Conv1D(filters=128, kernel_size=3, strides=1, padding=\"causal\", activation=\"tanh\")(conv_3_tower)\n",
    "    # Size 1 -> size 5 kernel conv of input (with tanh activation)\n",
    "    conv_5_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_5_tower = Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"tanh\")(conv_5_tower)\n",
    "    # Size 1 -> size 7 kernel conv of input (with tanh activation)\n",
    "    conv_7_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_7_tower = Conv1D(filters=128, kernel_size=7, strides=1, padding=\"causal\", activation=\"tanh\")(conv_7_tower)\n",
    "    # Concatenate all activation images\n",
    "    return concatenate([skip, conv_1_tower, conv_3_tower, conv_5_tower, conv_7_tower], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_12 (None, 12000, 1)\n",
      "conv1d_432 (None, 4000, 32)\n",
      "conv1d_433 (None, 2000, 64)\n",
      "conv1d_434 (None, 1000, 128)\n",
      "dropout_61 (None, 1000, 128)\n",
      "conv1d_435 (None, 1000, 128)\n",
      "conv1d_437 (None, 1000, 1)\n",
      "conv1d_439 (None, 1000, 1)\n",
      "conv1d_441 (None, 1000, 1)\n",
      "lambda_51 (None, 1000, 128)\n",
      "conv1d_436 (None, 1000, 128)\n",
      "conv1d_438 (None, 1000, 128)\n",
      "conv1d_440 (None, 1000, 128)\n",
      "conv1d_442 (None, 1000, 128)\n",
      "concatenate_51 (None, 1000, 640)\n",
      "dropout_62 (None, 1000, 640)\n",
      "conv1d_443 (None, 1000, 128)\n",
      "conv1d_445 (None, 1000, 1)\n",
      "conv1d_447 (None, 1000, 1)\n",
      "conv1d_449 (None, 1000, 1)\n",
      "lambda_52 (None, 1000, 128)\n",
      "conv1d_444 (None, 1000, 128)\n",
      "conv1d_446 (None, 1000, 128)\n",
      "conv1d_448 (None, 1000, 128)\n",
      "conv1d_450 (None, 1000, 128)\n",
      "concatenate_52 (None, 1000, 640)\n",
      "dropout_63 (None, 1000, 640)\n",
      "conv1d_451 (None, 1000, 128)\n",
      "conv1d_453 (None, 1000, 1)\n",
      "conv1d_455 (None, 1000, 1)\n",
      "conv1d_457 (None, 1000, 1)\n",
      "lambda_53 (None, 1000, 128)\n",
      "conv1d_452 (None, 1000, 128)\n",
      "conv1d_454 (None, 1000, 128)\n",
      "conv1d_456 (None, 1000, 128)\n",
      "conv1d_458 (None, 1000, 128)\n",
      "concatenate_53 (None, 1000, 640)\n",
      "dropout_64 (None, 1000, 640)\n",
      "conv1d_459 (None, 1000, 128)\n",
      "conv1d_461 (None, 1000, 1)\n",
      "conv1d_463 (None, 1000, 1)\n",
      "conv1d_465 (None, 1000, 1)\n",
      "lambda_54 (None, 1000, 128)\n",
      "conv1d_460 (None, 1000, 128)\n",
      "conv1d_462 (None, 1000, 128)\n",
      "conv1d_464 (None, 1000, 128)\n",
      "conv1d_466 (None, 1000, 128)\n",
      "concatenate_54 (None, 1000, 640)\n",
      "dropout_65 (None, 1000, 640)\n",
      "conv1d_467 (None, 1000, 128)\n",
      "conv1d_469 (None, 1000, 1)\n",
      "conv1d_471 (None, 1000, 1)\n",
      "conv1d_473 (None, 1000, 1)\n",
      "lambda_55 (None, 1000, 128)\n",
      "conv1d_468 (None, 1000, 128)\n",
      "conv1d_470 (None, 1000, 128)\n",
      "conv1d_472 (None, 1000, 128)\n",
      "conv1d_474 (None, 1000, 128)\n",
      "concatenate_55 (None, 1000, 640)\n",
      "dropout_66 (None, 1000, 640)\n",
      "flatten_2 (None, 640000)\n",
      "dense_2 (None, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reusable dilated convolution / inception module / dropout layer\n",
    "def DilatedInceptionModuleLayer(model, drop_rate):\n",
    "    model = Conv1D(filters=128, kernel_size=1, padding=\"causal\", dilation_rate=2)(model)\n",
    "    model = InceptionModule(model)\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "dim_rates = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Structure\n",
    "\"\"\"\n",
    "Rationale: \n",
    "\n",
    "3 \"CausalConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution space.\n",
    "- Providing downscaling\n",
    "(Feature extraction, while preserving temporal relationships)\n",
    "\n",
    "Then \"DilatedInceptionModule\" layers which retain the size of the sample space while extracting more features.\n",
    "\n",
    "- Using Convolutions to downsample from LeNet (?)\n",
    "- Dropout paper\n",
    "- ResNet for skip connections\n",
    "- Inception module adapted from GoogLeNet\n",
    "- Causal convolutions from WaveNet\n",
    "\"\"\"\n",
    "data = Input(shape=(12000, 1))\n",
    "cnn = Conv1D(filters=32, kernel_size=7, strides=3, padding=\"causal\", dilation_rate=1, activation=\"tanh\")(data)\n",
    "cnn = Conv1D(filters=64, kernel_size=7, strides=2, padding=\"causal\", dilation_rate=1, activation=\"tanh\")(cnn)\n",
    "cnn = Conv1D(filters=128, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, activation=\"tanh\")(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for drop_rate in dim_rates:\n",
    "    cnn = DilatedInceptionModuleLayer(cnn, drop_rate)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(10, activation='sigmoid')(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# Tensorboard logs\n",
    "tb_logs = TensorBoard(log_dir=\"logs/{}\".format(timestamp()))\n",
    "\n",
    "# Compile with stocastic gradient descent and mean squared error loss (same as multilabelled paper)\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "159/159 [==============================] - 3290s 21s/step - loss: 0.1396 - val_loss: 0.1158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f75922f98>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit_generator(generator=generators[\"training\"],\n",
    "                   validation_data=generators[\"test\"],\n",
    "                   callbacks=[tb_logs])\n",
    "#use_multiprocessing=use_multiprocessing,\n",
    "#workers=6,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"modelA.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"modelA.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
