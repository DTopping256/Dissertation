{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A (time sequence input)\n",
    "## With: one-hot labelling and binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Add, concatenate, Conv1D, Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.losses import binary_crossentropy, kullback_leibler_divergence\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from custom_metric import rounded_all_or_nothing_acc as RAON_accuracy\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, multilabelled_labels_to_ys, multilabelled_ys_to_labels, onehot_superclass_labels_to_ys, onehot_superclass_ys_to_labels, MULTI_LABEL, ONE_HOT, TIME_SEQUENCE, LINEAR_SPECTROGRAM, LOG_SPECTROGRAM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from keras.callbacks import TensorBoard\n",
    "from time_callback import Time_Callback\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1326798907680888862, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9821658354271242345\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 1},\n",
    "                        log_device_placement = True,\n",
    "                        gpu_options=gpu_options\n",
    "                       )\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "#Loss flags\n",
    "BINARY_CROSSENTROPY = 0\n",
    "KL_DIVERGENCE = 1\n",
    "LOSSES = {BINARY_CROSSENTROPY: binary_crossentropy, KL_DIVERGENCE: kullback_leibler_divergence}\n",
    "\n",
    "# Model config\n",
    "problem_type = ONE_HOT\n",
    "input_type = TIME_SEQUENCE\n",
    "loss = BINARY_CROSSENTROPY\n",
    "optimizer = \"adam\"\n",
    "\n",
    "# Name of model save and log\n",
    "problem_type_str = \"OneHot\" if problem_type == ONE_HOT else \"MultiHot\"\n",
    "input_type_str = {TIME_SEQUENCE: \"(1D)\", LINEAR_SPECTROGRAM: \"(linear 2D)\", LOG_SPECTROGRAM: \"(log 2D)\"}[input_type]  \n",
    "loss_str = \"BCE\" if loss == BINARY_CROSSENTROPY else \"KLD\"\n",
    "optimizer_str = optimizer.upper()\n",
    "date_time_str = str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "model_name = \"ModelA-{}-{}-{}-{}_{}\".format(input_type_str, problem_type_str, loss_str, optimizer_str, date_time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "batch_size = 50\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True, problem_type=problem_type, input_type=input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen validation\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen test\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 87)\n"
     ]
    }
   ],
   "source": [
    "batch_x_shape = None\n",
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_x_shape = batch_0[0].shape\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "### Model structure\n",
    "\n",
    "4 1D casual conv convolution layers which reduce the size of the sample space while increasing the size of the convolution/feature space.\n",
    "\n",
    "Creates feature space of 32, while downscaling the sample space to 500. Compared to 12000, total tensor sizes: 12000 -> 16000 (increase in data).\n",
    "\n",
    "After convoluton layers, LeakyReLU was used for activation because ReLU has been shown to perform well and LeakyReLU takes negatives into account slightly which appear in the data. For this reason He-normal was used to initialise the convolution kernals as this performs well with ReLU.\n",
    "\n",
    "Then 3 lots of \"DilatedDropoutSkipModule\" which retain the size of the sample space while extracting more features. With skip connections preserving earlier features. Finished with dropout layers to aid in generalisation during training.\n",
    "\n",
    "Flattened and passed to a fully-connected (dense layer) which reshapes the network into the output shape.\n",
    "\n",
    "Softmax activation layer for one-hot classification.\n",
    "\n",
    "Techniques from lit review:\n",
    "- LeNet: Convolutions with `stride > 1` to downscale sample space.\n",
    "- Using 1x1 convolution layers to downscale feature space, instead of pooling layers. \n",
    "- Leaky ReLU & He kernal inits.\n",
    "- Dropout for generalisation during training.\n",
    "- ResNet for skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "def InceptionModule(model):\n",
    "    # Size 1 kernel conv of input (with tanh activation)\n",
    "    conv_1_tower = Conv1D(filters=32, kernel_size=1, strides=1, padding=\"valid\", kernel_initializer='glorot_normal', activation=\"tanh\")(model)\n",
    "    # Size 1 -> size 3 kernel conv of input (with tanh activation)\n",
    "    conv_3_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_3_tower = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_3_tower)\n",
    "    # Size 1 -> size 5 kernel conv of input (with tanh activation)\n",
    "    conv_5_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_5_tower = Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_5_tower)\n",
    "    # Size 1 -> size 7 kernel conv of input (with tanh activation)\n",
    "    conv_7_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_7_tower = Conv1D(filters=32, kernel_size=7, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_7_tower)\n",
    "    # Concatenate all activation images\n",
    "    return concatenate([conv_1_tower, conv_3_tower, conv_5_tower, conv_7_tower], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 12000, 1)\n",
      "conv1d_1 (None, 4000, 8)\n",
      "conv1d_2 (None, 2000, 16)\n",
      "activation_1 (None, 2000, 16)\n",
      "conv1d_3 (None, 1000, 32)\n",
      "activation_2 (None, 1000, 32)\n",
      "conv1d_4 (None, 500, 32)\n",
      "activation_3 (None, 500, 32)\n",
      "dropout_1 (None, 500, 32)\n",
      "conv1d_5 (None, 500, 32)\n",
      "conv1d_7 (None, 500, 1)\n",
      "conv1d_9 (None, 500, 1)\n",
      "conv1d_11 (None, 500, 1)\n",
      "conv1d_6 (None, 500, 32)\n",
      "conv1d_8 (None, 500, 32)\n",
      "conv1d_10 (None, 500, 32)\n",
      "conv1d_12 (None, 500, 32)\n",
      "concatenate_1 (None, 500, 128)\n",
      "conv1d_13 (None, 500, 32)\n",
      "add_1 (None, 500, 32)\n",
      "dropout_2 (None, 500, 32)\n",
      "conv1d_14 (None, 500, 32)\n",
      "conv1d_16 (None, 500, 1)\n",
      "conv1d_18 (None, 500, 1)\n",
      "conv1d_20 (None, 500, 1)\n",
      "conv1d_15 (None, 500, 32)\n",
      "conv1d_17 (None, 500, 32)\n",
      "conv1d_19 (None, 500, 32)\n",
      "conv1d_21 (None, 500, 32)\n",
      "concatenate_2 (None, 500, 128)\n",
      "conv1d_22 (None, 500, 32)\n",
      "add_2 (None, 500, 32)\n",
      "dropout_3 (None, 500, 32)\n",
      "conv1d_23 (None, 500, 32)\n",
      "conv1d_25 (None, 500, 1)\n",
      "conv1d_27 (None, 500, 1)\n",
      "conv1d_29 (None, 500, 1)\n",
      "conv1d_24 (None, 500, 32)\n",
      "conv1d_26 (None, 500, 32)\n",
      "conv1d_28 (None, 500, 32)\n",
      "conv1d_30 (None, 500, 32)\n",
      "concatenate_3 (None, 500, 128)\n",
      "conv1d_31 (None, 500, 32)\n",
      "add_3 (None, 500, 32)\n",
      "dropout_4 (None, 500, 32)\n",
      "flatten_1 (None, 16000)\n",
      "dense_1 (None, 87)\n"
     ]
    }
   ],
   "source": [
    "# Reusable dilated convolution / inception module / dropout layer\n",
    "def DilatedInceptionModule(og_model, drop_rate):\n",
    "    model = Conv1D(filters=32, kernel_size=3, padding=\"causal\", dilation_rate=2, kernel_initializer='glorot_normal', activation=\"tanh\")(og_model)\n",
    "    model = InceptionModule(model)\n",
    "    # As concatenated layer produced 64 filters 1x1 down to 16 (same shape as skipped).\n",
    "    model = Conv1D(filters=32, kernel_size=1, padding=\"valid\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")(model)\n",
    "    # Skip connection\n",
    "    model = Add()([og_model, model])\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "# The dropout rates and amount of modules.\n",
    "NMODULES = 3\n",
    "DROP_RATES = [0.15, 0.2, 0.25]\n",
    "\n",
    "# Structure\n",
    "\"\"\"\n",
    "Rationale: \n",
    "\n",
    "3 \"CausalConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution space.\n",
    "- Providing downscaling\n",
    "(Feature extraction, while preserving temporal relationships)\n",
    "\n",
    "Then \"DilatedInceptionModule\" which retain the size of the sample space while extracting more features.\n",
    "\n",
    "- Using Convolutions to downsample from LeNet (?)\n",
    "- Dropout paper\n",
    "- ResNet for skip connections\n",
    "- Inception module adapted from GoogLeNet\n",
    "- Causal convolutions from WaveNet\n",
    "\"\"\"\n",
    "data = Input(shape=(12000, 1))\n",
    "cnn = Conv1D(filters=8, kernel_size=7, strides=3, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal')(data)\n",
    "cnn = Conv1D(filters=16, kernel_size=7, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal')(cnn)\n",
    "cnn = Activation(\"tanh\")(cnn)\n",
    "cnn = Conv1D(filters=32, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal')(cnn)\n",
    "cnn = Activation(\"tanh\")(cnn)\n",
    "cnn = Conv1D(filters=32, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal')(cnn)\n",
    "cnn = Activation(\"tanh\")(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for i in range(NMODULES):\n",
    "    cnn = DilatedInceptionModule(cnn, DROP_RATES[i])\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(batch_y_shape[1], kernel_initializer='glorot_normal', activation='sigmoid')(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# The label specific metric, dependant on the problem type from custom settings.\n",
    "problem_metric = {ONE_HOT: categorical_accuracy, MULTI_LABEL: binary_accuracy}[problem_type]\n",
    "    \n",
    "# Compile with custom settings defined earlier\n",
    "model.compile(optimizer=optimizer, loss=LOSSES[loss], metrics=[problem_metric, RAON_accuracy, LOSSES[(loss + 1)%2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1587/1587 [==============================] - 2532s 2s/step - loss: 0.0717 - categorical_accuracy: 0.0283 - rounded_all_or_nothing_acc: 0.0012 - kullback_leibler_divergence: 5.0109 - val_loss: 0.0607 - val_categorical_accuracy: 0.0712 - val_rounded_all_or_nothing_acc: 0.0155 - val_kullback_leibler_divergence: 4.3056\n",
      "Epoch 2/10\n",
      "1587/1587 [==============================] - 3795s 2s/step - loss: 0.0451 - categorical_accuracy: 0.3140 - rounded_all_or_nothing_acc: 0.1061 - kullback_leibler_divergence: 3.0052 - val_loss: 0.0359 - val_categorical_accuracy: 0.4610 - val_rounded_all_or_nothing_acc: 0.1942 - val_kullback_leibler_divergence: 2.3685\n",
      "Epoch 3/10\n",
      "1587/1587 [==============================] - 3781s 2s/step - loss: 0.0331 - categorical_accuracy: 0.5029 - rounded_all_or_nothing_acc: 0.2551 - kullback_leibler_divergence: 2.0885 - val_loss: 0.0266 - val_categorical_accuracy: 0.6165 - val_rounded_all_or_nothing_acc: 0.3907 - val_kullback_leibler_divergence: 1.4922\n",
      "Epoch 4/10\n",
      "1587/1587 [==============================] - 3780s 2s/step - loss: 0.0264 - categorical_accuracy: 0.6141 - rounded_all_or_nothing_acc: 0.3753 - kullback_leibler_divergence: 1.6069 - val_loss: 0.0223 - val_categorical_accuracy: 0.6923 - val_rounded_all_or_nothing_acc: 0.4919 - val_kullback_leibler_divergence: 1.2559\n",
      "Epoch 5/10\n",
      "1587/1587 [==============================] - 3781s 2s/step - loss: 0.0227 - categorical_accuracy: 0.6788 - rounded_all_or_nothing_acc: 0.4533 - kullback_leibler_divergence: 1.3502 - val_loss: 0.0202 - val_categorical_accuracy: 0.7371 - val_rounded_all_or_nothing_acc: 0.5468 - val_kullback_leibler_divergence: 1.0402\n",
      "Epoch 6/10\n",
      "1587/1587 [==============================] - 3839s 2s/step - loss: 0.0198 - categorical_accuracy: 0.7239 - rounded_all_or_nothing_acc: 0.5121 - kullback_leibler_divergence: 1.1579 - val_loss: 0.0176 - val_categorical_accuracy: 0.7777 - val_rounded_all_or_nothing_acc: 0.5910 - val_kullback_leibler_divergence: 1.0240\n",
      "Epoch 7/10\n",
      "1587/1587 [==============================] - 3782s 2s/step - loss: 0.0177 - categorical_accuracy: 0.7610 - rounded_all_or_nothing_acc: 0.5593 - kullback_leibler_divergence: 1.0149 - val_loss: 0.0165 - val_categorical_accuracy: 0.8012 - val_rounded_all_or_nothing_acc: 0.6440 - val_kullback_leibler_divergence: 0.8679\n",
      "Epoch 8/10\n",
      "1587/1587 [==============================] - 3785s 2s/step - loss: 0.0159 - categorical_accuracy: 0.7887 - rounded_all_or_nothing_acc: 0.5990 - kullback_leibler_divergence: 0.9051 - val_loss: 0.0151 - val_categorical_accuracy: 0.8203 - val_rounded_all_or_nothing_acc: 0.6624 - val_kullback_leibler_divergence: 0.9394\n",
      "Epoch 9/10\n",
      "1587/1587 [==============================] - 3783s 2s/step - loss: 0.0146 - categorical_accuracy: 0.8122 - rounded_all_or_nothing_acc: 0.6305 - kullback_leibler_divergence: 0.8225 - val_loss: 0.0141 - val_categorical_accuracy: 0.8340 - val_rounded_all_or_nothing_acc: 0.6896 - val_kullback_leibler_divergence: 0.8530\n",
      "Epoch 10/10\n",
      "1587/1587 [==============================] - 3779s 2s/step - loss: 0.0135 - categorical_accuracy: 0.8296 - rounded_all_or_nothing_acc: 0.6570 - kullback_leibler_divergence: 0.7496 - val_loss: 0.0135 - val_categorical_accuracy: 0.8494 - val_rounded_all_or_nothing_acc: 0.7062 - val_kullback_leibler_divergence: 0.8710\n"
     ]
    }
   ],
   "source": [
    "# Training logs\n",
    "log_dir = \"logs/{}\".format(model_name)\n",
    "# Tensorboard log\n",
    "tb_log = TensorBoard(log_dir=log_dir)\n",
    "# Custom time log\n",
    "time_log = Time_Callback(log_dir=log_dir)\n",
    "\n",
    "# Train model\n",
    "epochs = 10\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int(N[\"training\"]*dataset_perc) // batch_size,\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int(N[\"validation\"]*dataset_perc) // batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_log, time_log],\n",
    "                use_multiprocessing = use_multiprocessing,\n",
    "                workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model\n",
    "\n",
    "After 10 epochs of training and validation with the training and validation data sets, a seperate evaluation run calculated the final accuracy of the model using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01370924547540419,\n",
       " 0.8503787668816971,\n",
       " 0.7126136188762207,\n",
       " 0.8756347950159883]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int(N[\"test\"]*dataset_perc) // batch_size,\n",
    "    use_multiprocessing = use_multiprocessing,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights to '/models' directory\n",
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of predictions using test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['mid_tom', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['mid_tom', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "\n",
      "Sample 1\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'low_tom', 'snare'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.011, 0.0, 0.0, 0.0, 0.0, 0.049, 0.069, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.933, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'low_tom', 'snare'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "\n",
      "Sample 2\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['mid_tom'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['mid_tom'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 3\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['low_tom', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['low_tom', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "\n",
      "Sample 4\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'ride'], 'tech_label': ['normal', 'normal', 'bell']}\n",
      "Prediction:\n",
      "\t[0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.03, 0.0, 0.005, 0.0, 0.0, 0.024, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n",
      "Sample 5\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'ride'], 'tech_label': ['normal', 'open', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.006, 0.681, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'ride'], 'tech_label': ['normal', 'open', 'normal']}\n",
      "\n",
      "Sample 6\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['low_tom', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.008, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.185, 0.0, 0.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['low_tom', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "\n",
      "Sample 7\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.001, 0.0, 0.001, 0.001, 0.0, 0.001, 0.0, 0.047, 0.473, 0.002, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.003, 0.001, 0.0, 0.0, 0.0, 0.011, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n",
      "Sample 8\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.001, 0.0, 0.0, 0.0, 0.021, 0.013, 0.008, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "\n",
      "Sample 9\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.006, 0.031, 0.014, 0.045, 0.009, 0.02, 0.022, 0.044, 0.423, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.001, 0.001, 0.002, 0.003, 0.002, 0.0, 0.0, 0.002, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical accuracy (avg): 1.0\n",
      "RAON accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "preds, trues = [], []\n",
    "for i in range(10):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, batch_x_shape[1], batch_x_shape[2])), batch_y_shape[1])\n",
    "    print(\"Sample\", i)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, onehot_superclass_ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], onehot_superclass_ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    preds.append(pred_y)\n",
    "    trues.append(y)\n",
    "\n",
    "preds_tensor = K.variable(np.array(preds))\n",
    "trues_tensor = K.variable(np.array(trues))\n",
    "problem_acc = K.eval(K.mean(problem_metric(trues_tensor, preds_tensor)))\n",
    "print(\"{} accuracy (avg): {}\".format(\"Categorical\" if problem_type is ONE_HOT else \"Binary\", problem_acc))\n",
    "print(\"RAON accuracy: \", K.eval(RAON_accuracy(trues_tensor, preds_tensor)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
