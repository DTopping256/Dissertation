{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, concatenate, Conv1D, Dense, Dropout, Flatten, Input, Lambda\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, labels_to_ys, ys_to_labels\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import hamming_loss\n",
    "from keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6181485328952189546, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6454802693830699026\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 0},\n",
    "                        log_device_placement = True,\n",
    "                        #gpu_options=gpu_options\n",
    "                       )\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "model_name = \"modelA-bce-adam_\" + str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filepath': 'audio_data\\\\training_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\training_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n",
      "[{'filepath': 'audio_data\\\\validation_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\validation_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n",
      "[{'filepath': 'audio_data\\\\test_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\test_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Data generators\n",
    "batch_size = 50\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    print(sample_metadata[0:2])\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_label': ['stick'], 'kit_label': ['mid_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'mid_tom'], 'tech_label': ['open', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'snare', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'low_tom'], 'tech_label': ['open', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'low_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'crash', 'low_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['high_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'mid_tom'], 'tech_label': ['bell', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'high_tom', 'ride'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'hi_hat'], 'tech_label': ['normal', 'open']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['high_tom', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "{'hit_label': ['stick'], 'kit_label': ['mid_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['bell']}\n",
      "{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'crash'], 'tech_label': ['bell', 'normal']}\n",
      "{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['high_tom', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'high_tom', 'low_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['crash'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'hi_hat'], 'tech_label': ['normal', 'open']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['high_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['snare'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'ride', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['crash'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'low_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['crash', 'snare'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'ride'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['snare', 'low_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'low_tom'], 'tech_label': ['normal', 'open', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'ride', 'crash'], 'tech_label': ['normal', 'bell', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'high_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['crash'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['low_tom', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'snare', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['high_tom', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'ride'], 'tech_label': ['normal', 'open', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['high_tom', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'snare'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['mid_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': [], 'kit_label': [], 'tech_label': []}\n"
     ]
    }
   ],
   "source": [
    "generators[\"training\"].on_epoch_end()\n",
    "batch_0 = generators[\"training\"].__getitem__(0)\n",
    "for y in batch_0[1]:\n",
    "    print(ys_to_labels(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 10)\n",
      "Gen validation\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 10)\n",
      "Gen test\n",
      "In shape: (50, 12000, 1) \n",
      "Out shape: (50, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "def InceptionModule(model):\n",
    "    # Skip connection (uses input in concat)\n",
    "    skip = Lambda(lambda x: x)(model)\n",
    "    # Size 1 kernel conv of input (with tanh activation)\n",
    "    conv_1_tower = Conv1D(filters=32, kernel_size=1, strides=1, padding=\"valid\", kernel_initializer='glorot_normal', activation=\"tanh\")(model)\n",
    "    # Size 1 -> size 3 kernel conv of input (with tanh activation)\n",
    "    conv_3_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_3_tower = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_3_tower)\n",
    "    # Size 1 -> size 5 kernel conv of input (with tanh activation)\n",
    "    conv_5_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_5_tower = Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_5_tower)\n",
    "    # Size 1 -> size 7 kernel conv of input (with tanh activation)\n",
    "    conv_7_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_7_tower = Conv1D(filters=32, kernel_size=7, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_7_tower)\n",
    "    # Concatenate all activation images\n",
    "    return concatenate([skip, conv_1_tower, conv_3_tower, conv_5_tower, conv_7_tower], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 12000, 1)\n",
      "conv1d_1 (None, 4000, 16)\n",
      "conv1d_2 (None, 2000, 32)\n",
      "conv1d_3 (None, 1000, 32)\n",
      "dropout_1 (None, 1000, 32)\n",
      "conv1d_4 (None, 1000, 32)\n",
      "conv1d_6 (None, 1000, 1)\n",
      "conv1d_8 (None, 1000, 1)\n",
      "conv1d_10 (None, 1000, 1)\n",
      "lambda_1 (None, 1000, 32)\n",
      "conv1d_5 (None, 1000, 32)\n",
      "conv1d_7 (None, 1000, 32)\n",
      "conv1d_9 (None, 1000, 32)\n",
      "conv1d_11 (None, 1000, 32)\n",
      "concatenate_1 (None, 1000, 160)\n",
      "dropout_2 (None, 1000, 160)\n",
      "conv1d_12 (None, 1000, 32)\n",
      "conv1d_14 (None, 1000, 1)\n",
      "conv1d_16 (None, 1000, 1)\n",
      "conv1d_18 (None, 1000, 1)\n",
      "lambda_2 (None, 1000, 32)\n",
      "conv1d_13 (None, 1000, 32)\n",
      "conv1d_15 (None, 1000, 32)\n",
      "conv1d_17 (None, 1000, 32)\n",
      "conv1d_19 (None, 1000, 32)\n",
      "concatenate_2 (None, 1000, 160)\n",
      "dropout_3 (None, 1000, 160)\n",
      "conv1d_20 (None, 1000, 32)\n",
      "conv1d_22 (None, 1000, 1)\n",
      "conv1d_24 (None, 1000, 1)\n",
      "conv1d_26 (None, 1000, 1)\n",
      "lambda_3 (None, 1000, 32)\n",
      "conv1d_21 (None, 1000, 32)\n",
      "conv1d_23 (None, 1000, 32)\n",
      "conv1d_25 (None, 1000, 32)\n",
      "conv1d_27 (None, 1000, 32)\n",
      "concatenate_3 (None, 1000, 160)\n",
      "dropout_4 (None, 1000, 160)\n",
      "conv1d_28 (None, 1000, 32)\n",
      "conv1d_30 (None, 1000, 1)\n",
      "conv1d_32 (None, 1000, 1)\n",
      "conv1d_34 (None, 1000, 1)\n",
      "lambda_4 (None, 1000, 32)\n",
      "conv1d_29 (None, 1000, 32)\n",
      "conv1d_31 (None, 1000, 32)\n",
      "conv1d_33 (None, 1000, 32)\n",
      "conv1d_35 (None, 1000, 32)\n",
      "concatenate_4 (None, 1000, 160)\n",
      "dropout_5 (None, 1000, 160)\n",
      "conv1d_36 (None, 1000, 32)\n",
      "conv1d_38 (None, 1000, 1)\n",
      "conv1d_40 (None, 1000, 1)\n",
      "conv1d_42 (None, 1000, 1)\n",
      "lambda_5 (None, 1000, 32)\n",
      "conv1d_37 (None, 1000, 32)\n",
      "conv1d_39 (None, 1000, 32)\n",
      "conv1d_41 (None, 1000, 32)\n",
      "conv1d_43 (None, 1000, 32)\n",
      "concatenate_5 (None, 1000, 160)\n",
      "dropout_6 (None, 1000, 160)\n",
      "flatten_1 (None, 160000)\n",
      "dense_1 (None, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reusable dilated convolution / inception module / dropout layer\n",
    "def DilatedInceptionModuleLayer(model, drop_rate):\n",
    "    model = Conv1D(filters=32, kernel_size=1, padding=\"causal\", dilation_rate=2, kernel_initializer='glorot_normal', activation=\"tanh\")(model)\n",
    "    model = InceptionModule(model)\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "dim_rates = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Structure\n",
    "\"\"\"\n",
    "Rationale: \n",
    "\n",
    "3 \"CausalConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution space.\n",
    "- Providing downscaling\n",
    "(Feature extraction, while preserving temporal relationships)\n",
    "\n",
    "Then \"DilatedInceptionModule\" layers which retain the size of the sample space while extracting more features.\n",
    "\n",
    "- Using Convolutions to downsample from LeNet (?)\n",
    "- Dropout paper\n",
    "- ResNet for skip connections\n",
    "- Inception module adapted from GoogLeNet\n",
    "- Causal convolutions from WaveNet\n",
    "\"\"\"\n",
    "data = Input(shape=(12000, 1))\n",
    "cnn = Conv1D(filters=16, kernel_size=7, strides=3, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(data)\n",
    "cnn = Conv1D(filters=32, kernel_size=7, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(cnn)\n",
    "cnn = Conv1D(filters=32, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for drop_rate in dim_rates:\n",
    "    cnn = DilatedInceptionModuleLayer(cnn, drop_rate)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(10, kernel_initializer='glorot_normal', activation='sigmoid')(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# Tensorboard logs\n",
    "tb_logs = TensorBoard(log_dir=\"logs/{}\".format(model_name))\n",
    "\n",
    "def hamming_loss(threshold, output_shape):\n",
    "    def hamming(y_true, y_pred):\n",
    "        thr = tf.fill(value=threshold, dims=output_shape)\n",
    "        y_pred_thresholded = K.cast(K.greater_equal(y_pred, thr), dtype=y_true.dtype)\n",
    "        dist_tensor = K.cast(K.not_equal(y_true, y_pred_thresholded), dtype=y_true.dtype)\n",
    "        dist = K.sum(dist_tensor)\n",
    "        return 1/(output_shape[0]*output_shape[1])*dist\n",
    "    return hamming\n",
    "\n",
    "hl = hamming_loss(0.5, batch_y_shape)\n",
    "\n",
    "# Compile with stocastic gradient descent and mean squared error loss (same as multilabelled paper)\n",
    "#run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "model.compile(optimizer=\"adam\", loss=hl, metrics=[\"accuracy\", \"binary_crossentropy\"])#, options=run_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "  37/1928 [..............................] - ETA: 2:42:00 - loss: 0.4796 - acc: 0.8146 - hamming: 0.1757"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d84a75b39197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"validation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset_perc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtb_logs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[1;31m#use_multiprocessing = use_multiprocessing,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;31m#workers = 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 5\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int((N[\"training\"]*dataset_perc) // batch_size),\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int((N[\"validation\"]*dataset_perc) / batch_size),\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_logs]\n",
    "                #use_multiprocessing = use_multiprocessing,\n",
    "                #workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int((N[\"test\"]*dataset_perc) // batch_size)\n",
    "    #use_multiprocessing = use_multiprocessing,\n",
    "    #workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(batch_size):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, 12000, 1)), 10)\n",
    "    print(pred_y.shape)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    predictions.append({\"pred\": pred_y, \"actual\": y})\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
