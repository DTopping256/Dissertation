{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model C (log spectrogram input)\n",
    "## With: one-hot labelling and binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Add, Conv2D, Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "from keras.metrics import categorical_accuracy\n",
    "from custom_metric import rounded_all_or_nothing_acc as RAON_accuracy\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, multilabelled_labels_to_ys, multilabelled_ys_to_labels, onehot_superclass_labels_to_ys, onehot_superclass_ys_to_labels, MULTI_LABEL, ONE_HOT, LOG_SPECTROGRAM, LINEAR_SPECTROGRAM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import hamming_loss\n",
    "from keras.callbacks import TensorBoard\n",
    "from time_callback import Time_Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13434804756218702198, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12967758687537894042\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This makes sure that Tensorflow has access to my GPU, as well as my CPU.\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 1},\n",
    "                        log_device_placement = True,\n",
    "                        gpu_options=gpu_options\n",
    "                       )\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "model_name = \"modelC-bce-adam_\" + str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "batch_size = 50\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True, problem_type=ONE_HOT, input_type=LOG_SPECTROGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen validation\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 87)\n",
      "Gen test\n",
      "In shape: (50, 129, 53, 1) \n",
      "Out shape: (50, 87)\n"
     ]
    }
   ],
   "source": [
    "# Check the input and output spaces of the generators on each data set\n",
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "### Model structure\n",
    "\n",
    "2 \"2DConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution/feature space. Causal convolutions don't exist for 2D (yet) so fully connected 2D convolution will suffice.\n",
    "\n",
    "Creates feature space of 32, while downscaling the sample space to 33x14 (462). Compared to 129x53 (6837), total tensor sizes: 341850 -> 739200 (increase in data).\n",
    "\n",
    "After convoluton layers, LeakyReLU was used for activation because ReLU has been shown to perform well and LeakyReLU takes negatives into account slightly which appear in the data. For this reason He-normal was used to initialise the convolution kernals as this performs well with ReLU.\n",
    "\n",
    "Then 3 lots of \"DilatedDropoutSkipModule\" which retain the size of the sample space while extracting more features. With skip connections preserving earlier features. Finished with dropout layers to aid in generalisation during training.\n",
    "\n",
    "Flattened and passed to a fully-connected (dense layer) which reshapes the network into the output shape.\n",
    "\n",
    "Softmax activation layer for one-hot classification.\n",
    "\n",
    "Techniques from lit review:\n",
    "- LeNet: Convolutions with `stride > 1` to downscale sample space.\n",
    "- Using 1x1 convolution layers to downscale feature space, instead of pooling layers. \n",
    "- Leaky ReLU & He kernal inits.\n",
    "- Dropout for generalisation during training.\n",
    "- ResNet for skip connections.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Binary cross-entropy was used for this one-hot encoded problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 129, 53, 1)\n",
      "conv2d_1 (None, 65, 27, 16)\n",
      "leaky_re_lu_1 (None, 65, 27, 16)\n",
      "conv2d_2 (None, 33, 14, 32)\n",
      "leaky_re_lu_2 (None, 33, 14, 32)\n",
      "dropout_1 (None, 33, 14, 32)\n",
      "conv2d_3 (None, 33, 14, 1)\n",
      "conv2d_4 (None, 33, 14, 8)\n",
      "conv2d_5 (None, 33, 14, 32)\n",
      "leaky_re_lu_3 (None, 33, 14, 32)\n",
      "add_1 (None, 33, 14, 32)\n",
      "dropout_2 (None, 33, 14, 32)\n",
      "conv2d_6 (None, 33, 14, 1)\n",
      "conv2d_7 (None, 33, 14, 8)\n",
      "conv2d_8 (None, 33, 14, 32)\n",
      "leaky_re_lu_4 (None, 33, 14, 32)\n",
      "add_2 (None, 33, 14, 32)\n",
      "dropout_3 (None, 33, 14, 32)\n",
      "conv2d_9 (None, 33, 14, 1)\n",
      "conv2d_10 (None, 33, 14, 8)\n",
      "conv2d_11 (None, 33, 14, 32)\n",
      "leaky_re_lu_5 (None, 33, 14, 32)\n",
      "add_3 (None, 33, 14, 32)\n",
      "dropout_4 (None, 33, 14, 32)\n",
      "flatten_1 (None, 14784)\n",
      "dense_1 (None, 87)\n",
      "activation_1 (None, 87)\n"
     ]
    }
   ],
   "source": [
    "# Reusable series of layers\n",
    "def DilatedDropoutSkipModule(og_model, drop_rate):\n",
    "    model = Conv2D(filters=1, kernel_size=1, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(og_model)\n",
    "    model = Conv2D(filters=8, kernel_size=3, padding=\"same\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = Conv2D(filters=32, kernel_size=3, padding=\"same\", dilation_rate=2, kernel_initializer='he_normal')(model)\n",
    "    model = LeakyReLU(0.3)(model)\n",
    "    model = Add()([og_model, model])\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "# The dropout rates and amount of modules.\n",
    "NMODULES = 3\n",
    "DROP_RATES = [0.15, 0.2, 0.25]\n",
    "\n",
    "# Structure\n",
    "data = Input(shape=(129, 53, 1))\n",
    "cnn = Conv2D(filters=16, kernel_size=5, strides=2, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(data)\n",
    "cnn = LeakyReLU(0.3)(cnn)\n",
    "cnn = Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", dilation_rate=1, kernel_initializer='he_normal')(cnn)\n",
    "cnn = LeakyReLU(0.3)(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for i in range(NMODULES):\n",
    "    cnn = DilatedDropoutSkipModule(cnn, DROP_RATES[i])\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(batch_y_shape[1], kernel_initializer='he_normal')(cnn)\n",
    "cnn = Activation(\"softmax\")(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# Compile with stocastic gradient descent and mean squared error loss (same as multilabelled paper)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[categorical_accuracy, RAON_accuracy, kullback_leibler_divergence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1587/1587 [==============================] - 1379s 869ms/step - loss: 0.0645 - categorical_accuracy: 0.0243 - rounded_all_or_nothing_acc: 0.0000e+00 - kullback_leibler_divergence: 4.6048 - val_loss: 0.0616 - val_categorical_accuracy: 0.0339 - val_rounded_all_or_nothing_acc: 0.0000e+00 - val_kullback_leibler_divergence: 4.3663\n",
      "Epoch 2/10\n",
      "1587/1587 [==============================] - 1400s 882ms/step - loss: 0.0444 - categorical_accuracy: 0.2774 - rounded_all_or_nothing_acc: 0.1250 - kullback_leibler_divergence: 2.9824 - val_loss: 0.0332 - val_categorical_accuracy: 0.4447 - val_rounded_all_or_nothing_acc: 0.2389 - val_kullback_leibler_divergence: 2.1154\n",
      "Epoch 3/10\n",
      "1587/1587 [==============================] - 1535s 967ms/step - loss: 0.0318 - categorical_accuracy: 0.4626 - rounded_all_or_nothing_acc: 0.2893 - kullback_leibler_divergence: 2.0102 - val_loss: 0.0277 - val_categorical_accuracy: 0.5366 - val_rounded_all_or_nothing_acc: 0.3463 - val_kullback_leibler_divergence: 1.7208\n",
      "Epoch 4/10\n",
      "1587/1587 [==============================] - 1435s 904ms/step - loss: 0.0271 - categorical_accuracy: 0.5426 - rounded_all_or_nothing_acc: 0.3845 - kullback_leibler_divergence: 1.6731 - val_loss: 0.0243 - val_categorical_accuracy: 0.5956 - val_rounded_all_or_nothing_acc: 0.4257 - val_kullback_leibler_divergence: 1.4799\n",
      "Epoch 5/10\n",
      "1587/1587 [==============================] - 1164s 734ms/step - loss: 0.0239 - categorical_accuracy: 0.5986 - rounded_all_or_nothing_acc: 0.4583 - kullback_leibler_divergence: 1.4484 - val_loss: 0.0220 - val_categorical_accuracy: 0.6322 - val_rounded_all_or_nothing_acc: 0.4874 - val_kullback_leibler_divergence: 1.3265\n",
      "Epoch 6/10\n",
      "1587/1587 [==============================] - 1200s 756ms/step - loss: 0.0214 - categorical_accuracy: 0.6414 - rounded_all_or_nothing_acc: 0.5181 - kullback_leibler_divergence: 1.2801 - val_loss: 0.0208 - val_categorical_accuracy: 0.6572 - val_rounded_all_or_nothing_acc: 0.5221 - val_kullback_leibler_divergence: 1.2445\n",
      "Epoch 7/10\n",
      "1587/1587 [==============================] - 1168s 736ms/step - loss: 0.0196 - categorical_accuracy: 0.6733 - rounded_all_or_nothing_acc: 0.5628 - kullback_leibler_divergence: 1.1550 - val_loss: 0.0193 - val_categorical_accuracy: 0.6827 - val_rounded_all_or_nothing_acc: 0.5645 - val_kullback_leibler_divergence: 1.1495\n",
      "Epoch 8/10\n",
      "1587/1587 [==============================] - 1199s 755ms/step - loss: 0.0178 - categorical_accuracy: 0.7050 - rounded_all_or_nothing_acc: 0.6054 - kullback_leibler_divergence: 1.0421 - val_loss: 0.0190 - val_categorical_accuracy: 0.6880 - val_rounded_all_or_nothing_acc: 0.5847 - val_kullback_leibler_divergence: 1.1249\n",
      "Epoch 9/10\n",
      "1587/1587 [==============================] - 1158s 729ms/step - loss: 0.0167 - categorical_accuracy: 0.7250 - rounded_all_or_nothing_acc: 0.6317 - kullback_leibler_divergence: 0.9632 - val_loss: 0.0179 - val_categorical_accuracy: 0.7077 - val_rounded_all_or_nothing_acc: 0.6131 - val_kullback_leibler_divergence: 1.0593\n",
      "Epoch 10/10\n",
      "1587/1587 [==============================] - 1212s 764ms/step - loss: 0.0154 - categorical_accuracy: 0.7447 - rounded_all_or_nothing_acc: 0.6614 - kullback_leibler_divergence: 0.8830 - val_loss: 0.0177 - val_categorical_accuracy: 0.7113 - val_rounded_all_or_nothing_acc: 0.6223 - val_kullback_leibler_divergence: 1.0448\n"
     ]
    }
   ],
   "source": [
    "# Training logs\n",
    "log_dir = \"logs/{}\".format(model_name)\n",
    "# Tensorboard log\n",
    "tb_log = TensorBoard(log_dir=log_dir)\n",
    "# Custom time log\n",
    "time_log = Time_Callback(log_dir=log_dir)\n",
    "\n",
    "# Train model\n",
    "epochs = 10\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int(N[\"training\"]*dataset_perc) // batch_size,\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int(N[\"validation\"]*dataset_perc) // batch_size,\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_log, time_log],\n",
    "                use_multiprocessing = use_multiprocessing,\n",
    "                workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model\n",
    "\n",
    "After 10 epochs of training and validation with the training and validation data sets, a seperate evaluation run calculated the final accuracy of the model using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.017459439035995027,\n",
       " 0.7154166511767968,\n",
       " 0.6264393794293882,\n",
       " 1.0263472116987968]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int(N[\"test\"]*dataset_perc) // batch_size,\n",
    "    use_multiprocessing = use_multiprocessing,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights to `/models` directory\n",
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of predictions using test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['crash', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.712, 0.0, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.003, 0.001, 0.001, 0.086, 0.001, 0.091, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['crash'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 1\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['crash', 'high_tom'], 'tech_label': ['normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.821, 0.064, 0.0, 0.006, 0.007, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.022, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['crash', 'high_tom'], 'tech_label': ['normal', 'normal']}\n",
      "\n",
      "Sample 2\n",
      "Actual:\n",
      "\t[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['hi_hat'], 'tech_label': ['open']}\n",
      "\n",
      "Sample 3\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'high_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.014, 0.01, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.002, 0.0, 0.0, 0.0, 0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043, 0.0, 0.0, 0.001, 0.0, 0.0, 0.002, 0.0, 0.431, 0.035, 0.015, 0.0, 0.356, 0.028, 0.002, 0.015, 0.0, 0.003, 0.014, 0.006, 0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n",
      "Sample 4\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.712, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.261, 0.001, 0.003, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'high_tom'], 'tech_label': ['normal', 'normal']}\n",
      "\n",
      "Sample 5\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.014, 0.0, 0.313, 0.008, 0.003, 0.0, 0.009, 0.073, 0.0, 0.0, 0.0, 0.006, 0.001, 0.004, 0.009, 0.001, 0.0, 0.001, 0.0, 0.03, 0.0, 0.019, 0.0, 0.001, 0.045, 0.0, 0.0, 0.0, 0.0, 0.001, 0.012, 0.0, 0.0, 0.001, 0.002, 0.0, 0.038, 0.0, 0.011, 0.0, 0.0, 0.16, 0.0, 0.001, 0.002, 0.039, 0.003, 0.0, 0.005, 0.002, 0.0, 0.008, 0.0, 0.001, 0.0, 0.0, 0.002, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.0, 0.002, 0.013, 0.002, 0.005, 0.015, 0.0, 0.008, 0.003, 0.001, 0.001, 0.001, 0.0, 0.02, 0.017, 0.0, 0.005, 0.049, 0.004, 0.001, 0.019],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n",
      "Sample 6\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'mid_tom'], 'tech_label': ['open', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078, 0.007, 0.871, 0.0, 0.001, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'mid_tom'], 'tech_label': ['open', 'normal']}\n",
      "\n",
      "Sample 7\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'high_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "Prediction:\n",
      "\t[0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.006, 0.0, 0.124, 0.001, 0.0, 0.0, 0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.004, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.396, 0.0, 0.0, 0.272, 0.007, 0.002, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.003, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n",
      "Sample 8\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.001, 0.0, 0.0, 0.0, 0.004, 0.938, 0.054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "\n",
      "Sample 9\n",
      "Actual:\n",
      "\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],\n",
      "\t{'hit_label': ['stick'], 'kit_label': ['snare'], 'tech_label': ['normal']}\n",
      "Prediction:\n",
      "\t[0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.089, 0.0, 0.0, 0.0, 0.001, 0.001, 0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.253, 0.0, 0.0, 0.0, 0.367, 0.0, 0.0, 0.005, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.004, 0.003, 0.0],\n",
      "\t{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical_accuracy (avg) 0.6\n",
      "RAON accuracy:  0.4\n"
     ]
    }
   ],
   "source": [
    "preds, trues = [], []\n",
    "for i in range(10):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, 129, 53, 1)), batch_y_shape[1])\n",
    "    print(\"Sample\", i)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, onehot_superclass_ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], onehot_superclass_ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    preds.append(pred_y)\n",
    "    trues.append(y)\n",
    "\n",
    "preds_tensor = K.variable(np.array(preds))\n",
    "trues_tensor = K.variable(np.array(trues))\n",
    "print(\"Categorical_accuracy (avg)\", K.eval(K.mean(categorical_accuracy(trues_tensor, preds_tensor))))\n",
    "print(\"RAON accuracy: \", K.eval(RAON_accuracy(trues_tensor, preds_tensor)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
