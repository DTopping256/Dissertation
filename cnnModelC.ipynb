{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read settings file...\n",
      "\tRead successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, concatenate, Conv1D, Dense, Dropout, Flatten, Input, Lambda\n",
    "from keras.models import Model\n",
    "from generator import AudioGenerator, kltls, labels_to_ys, ys_to_labels\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import hamming_loss\n",
    "from keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allows me to import my modules\n",
    "sys.path.append('./modules')\n",
    "from audio_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16700123651963006759, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 577778483\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 1395718361558479677\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells Tensorflow to use the GPU\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 0},\n",
    "                        log_device_placement = True\n",
    "                       )\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "model_name = \"modelA-bce-adam_\" + str(datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filepath': 'audio_data\\\\training_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\training_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n",
      "[{'filepath': 'audio_data\\\\validation_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\validation_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n",
      "[{'filepath': 'audio_data\\\\test_data\\\\beater\\\\bass_drum\\\\normal\\\\0.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}, {'filepath': 'audio_data\\\\test_data\\\\beater\\\\bass_drum\\\\normal\\\\1.gz', 'labels': {'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}, 'augmentations': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Data generators\n",
    "batch_size = 40\n",
    "generators = {\"training\": None, \"validation\": None, \"test\": None}\n",
    "N = {\"training\": 0, \"validation\": 0, \"test\": 0}\n",
    "for data_type in generators.keys():\n",
    "    sample_metadata = get_file_classes(data_type)\n",
    "    print(sample_metadata[0:2])\n",
    "    N[data_type] = len(sample_metadata)\n",
    "    filenames = [sm[\"filepath\"] for sm in sample_metadata]\n",
    "    labels = [sm[\"labels\"] for sm in sample_metadata]\n",
    "    generators[data_type] = AudioGenerator(filenames, labels, data_type, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'low_tom', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'crash'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'hi_hat'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'snare'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'high_tom', 'snare'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'low_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'ride'], 'tech_label': ['open', 'bell']}\n",
      "{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'crash', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'ride'], 'tech_label': ['normal', 'open', 'bell']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'snare'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'high_tom', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'mid_tom'], 'tech_label': ['normal', 'open', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'mid_tom'], 'tech_label': ['bell', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'mid_tom'], 'tech_label': ['bell', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'snare'], 'tech_label': ['open', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['bell']}\n",
      "{'hit_label': ['beater'], 'kit_label': ['bass_drum'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['low_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'snare', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['hi_hat', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'low_tom'], 'tech_label': ['bell', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'mid_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['snare'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['snare'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['snare', 'low_tom'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'high_tom', 'ride'], 'tech_label': ['normal', 'normal', 'bell']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['high_tom'], 'tech_label': ['normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'low_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'ride'], 'tech_label': ['normal', 'bell']}\n",
      "{'hit_label': [], 'kit_label': [], 'tech_label': []}\n",
      "{'hit_label': ['beater', 'stick'], 'kit_label': ['bass_drum', 'hi_hat'], 'tech_label': ['normal', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'ride', 'low_tom'], 'tech_label': ['normal', 'bell', 'normal']}\n",
      "{'hit_label': ['beater', 'stick', 'stick'], 'kit_label': ['bass_drum', 'hi_hat', 'mid_tom'], 'tech_label': ['normal', 'normal', 'normal']}\n",
      "{'hit_label': ['stick'], 'kit_label': ['ride'], 'tech_label': ['bell']}\n",
      "{'hit_label': ['stick', 'stick'], 'kit_label': ['ride', 'snare'], 'tech_label': ['bell', 'normal']}\n"
     ]
    }
   ],
   "source": [
    "generators[\"training\"].on_epoch_end()\n",
    "batch_0 = generators[\"training\"].__getitem__(0)\n",
    "for y in batch_0[1]:\n",
    "    print(ys_to_labels(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen training\n",
      "In shape: (40, 12000, 1) \n",
      "Out shape: (40, 10)\n",
      "Gen validation\n",
      "In shape: (40, 12000, 1) \n",
      "Out shape: (40, 10)\n",
      "Gen test\n",
      "In shape: (40, 12000, 1) \n",
      "Out shape: (40, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_y_shape = None \n",
    "for name, gen in generators.items():\n",
    "    print(\"Gen\", name)\n",
    "    batch_0 = gen.__getitem__(0)\n",
    "    print(\"In shape:\", batch_0[0].shape, \"\\nOut shape:\", batch_0[1].shape)\n",
    "    batch_y_shape = batch_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picklable: True\n"
     ]
    }
   ],
   "source": [
    "# Test whether generator arguments are picklable (whether they can be multiprocessed)\n",
    "use_multiprocessing = True\n",
    "for gen in generators:\n",
    "    try:\n",
    "        pickle.dumps(gen)\n",
    "    except:\n",
    "        print(sys.exc_info())\n",
    "        use_multiprocessing = False\n",
    "        break\n",
    "print(\"Picklable:\", use_multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "def InceptionModule(model):\n",
    "    # Skip connection (uses input in concat)\n",
    "    skip = Lambda(lambda x: x)(model)\n",
    "    # Size 1 kernel conv of input (with tanh activation)\n",
    "    conv_1_tower = Conv1D(filters=32, kernel_size=1, strides=1, padding=\"valid\", kernel_initializer='glorot_normal', activation=\"tanh\")(model)\n",
    "    # Size 1 -> size 3 kernel conv of input (with tanh activation)\n",
    "    conv_3_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_3_tower = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_3_tower)\n",
    "    # Size 1 -> size 5 kernel conv of input (with tanh activation)\n",
    "    conv_5_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_5_tower = Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_5_tower)\n",
    "    # Size 1 -> size 7 kernel conv of input (with tanh activation)\n",
    "    conv_7_tower = Conv1D(filters=1, kernel_size=1, strides=1, padding=\"valid\")(model)\n",
    "    conv_7_tower = Conv1D(filters=32, kernel_size=7, strides=1, padding=\"causal\", kernel_initializer='glorot_normal', activation=\"tanh\")(conv_7_tower)\n",
    "    # Concatenate all activation images\n",
    "    return concatenate([skip, conv_1_tower, conv_3_tower, conv_5_tower, conv_7_tower], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "input_1 (None, 12000, 1)\n",
      "conv1d_1 (None, 4000, 16)\n",
      "conv1d_2 (None, 2000, 32)\n",
      "conv1d_3 (None, 1000, 32)\n",
      "dropout_1 (None, 1000, 32)\n",
      "conv1d_4 (None, 1000, 32)\n",
      "conv1d_6 (None, 1000, 1)\n",
      "conv1d_8 (None, 1000, 1)\n",
      "conv1d_10 (None, 1000, 1)\n",
      "lambda_1 (None, 1000, 32)\n",
      "conv1d_5 (None, 1000, 32)\n",
      "conv1d_7 (None, 1000, 32)\n",
      "conv1d_9 (None, 1000, 32)\n",
      "conv1d_11 (None, 1000, 32)\n",
      "concatenate_1 (None, 1000, 160)\n",
      "dropout_2 (None, 1000, 160)\n",
      "conv1d_12 (None, 1000, 32)\n",
      "conv1d_14 (None, 1000, 1)\n",
      "conv1d_16 (None, 1000, 1)\n",
      "conv1d_18 (None, 1000, 1)\n",
      "lambda_2 (None, 1000, 32)\n",
      "conv1d_13 (None, 1000, 32)\n",
      "conv1d_15 (None, 1000, 32)\n",
      "conv1d_17 (None, 1000, 32)\n",
      "conv1d_19 (None, 1000, 32)\n",
      "concatenate_2 (None, 1000, 160)\n",
      "dropout_3 (None, 1000, 160)\n",
      "conv1d_20 (None, 1000, 32)\n",
      "conv1d_22 (None, 1000, 1)\n",
      "conv1d_24 (None, 1000, 1)\n",
      "conv1d_26 (None, 1000, 1)\n",
      "lambda_3 (None, 1000, 32)\n",
      "conv1d_21 (None, 1000, 32)\n",
      "conv1d_23 (None, 1000, 32)\n",
      "conv1d_25 (None, 1000, 32)\n",
      "conv1d_27 (None, 1000, 32)\n",
      "concatenate_3 (None, 1000, 160)\n",
      "dropout_4 (None, 1000, 160)\n",
      "conv1d_28 (None, 1000, 32)\n",
      "conv1d_30 (None, 1000, 1)\n",
      "conv1d_32 (None, 1000, 1)\n",
      "conv1d_34 (None, 1000, 1)\n",
      "lambda_4 (None, 1000, 32)\n",
      "conv1d_29 (None, 1000, 32)\n",
      "conv1d_31 (None, 1000, 32)\n",
      "conv1d_33 (None, 1000, 32)\n",
      "conv1d_35 (None, 1000, 32)\n",
      "concatenate_4 (None, 1000, 160)\n",
      "dropout_5 (None, 1000, 160)\n",
      "conv1d_36 (None, 1000, 32)\n",
      "conv1d_38 (None, 1000, 1)\n",
      "conv1d_40 (None, 1000, 1)\n",
      "conv1d_42 (None, 1000, 1)\n",
      "lambda_5 (None, 1000, 32)\n",
      "conv1d_37 (None, 1000, 32)\n",
      "conv1d_39 (None, 1000, 32)\n",
      "conv1d_41 (None, 1000, 32)\n",
      "conv1d_43 (None, 1000, 32)\n",
      "concatenate_5 (None, 1000, 160)\n",
      "dropout_6 (None, 1000, 160)\n",
      "flatten_1 (None, 160000)\n",
      "dense_1 (None, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reusable dilated convolution / inception module / dropout layer\n",
    "def DilatedInceptionModuleLayer(model, drop_rate):\n",
    "    model = Conv1D(filters=32, kernel_size=1, padding=\"causal\", dilation_rate=2, kernel_initializer='glorot_normal', activation=\"tanh\")(model)\n",
    "    model = InceptionModule(model)\n",
    "    return Dropout(rate=drop_rate)(model)\n",
    "\n",
    "dim_rates = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Structure\n",
    "\"\"\"\n",
    "Rationale: \n",
    "\n",
    "3 \"CausalConvAct\" convolution layers which reduce the size of the sample space while increasing the size of the convolution space.\n",
    "- Providing downscaling\n",
    "(Feature extraction, while preserving temporal relationships)\n",
    "\n",
    "Then \"DilatedInceptionModule\" layers which retain the size of the sample space while extracting more features.\n",
    "\n",
    "- Using Convolutions to downsample from LeNet (?)\n",
    "- Dropout paper\n",
    "- ResNet for skip connections\n",
    "- Inception module adapted from GoogLeNet\n",
    "- Causal convolutions from WaveNet\n",
    "\"\"\"\n",
    "data = Input(shape=(12000, 1))\n",
    "cnn = Conv1D(filters=16, kernel_size=7, strides=3, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(data)\n",
    "cnn = Conv1D(filters=32, kernel_size=7, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(cnn)\n",
    "cnn = Conv1D(filters=32, kernel_size=5, strides=2, padding=\"causal\", dilation_rate=1, kernel_initializer='glorot_normal', activation=\"tanh\")(cnn)\n",
    "cnn = Dropout(rate=0.1)(cnn)\n",
    "for drop_rate in dim_rates:\n",
    "    cnn = DilatedInceptionModuleLayer(cnn, drop_rate)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(10, kernel_initializer='glorot_normal', activation='sigmoid')(cnn)\n",
    "model = Model(inputs=data, outputs=cnn)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)\n",
    "\n",
    "# Tensorboard logs\n",
    "tb_logs = TensorBoard(log_dir=\"logs/{}\".format(model_name))\n",
    "\n",
    "def hamming_loss(threshold, output_shape):\n",
    "    def hamming(y_true, y_pred):\n",
    "        thr = tf.fill(value=threshold, dims=output_shape)\n",
    "        y_pred_thresholded = K.cast(K.greater_equal(y_pred, thr), dtype=y_true.dtype)\n",
    "        dist_tensor = K.cast(K.not_equal(y_true, y_pred_thresholded), dtype=y_true.dtype)\n",
    "        dist = K.sum(dist_tensor)\n",
    "        return 1/(output_shape[0]*output_shape[1])*dist\n",
    "    return hamming\n",
    "\n",
    "# Compile with stocastic gradient descent and mean squared error loss (same as multilabelled paper)\n",
    "#run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", hamming_loss(0.7, batch_y_shape)])#, options=run_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "2410/2410 [==============================] - 8776s 4s/step - loss: 0.4388 - acc: 0.8322 - hamming: 0.1719 - val_loss: 0.4024 - val_acc: 0.8468 - val_hamming: 0.1572\n",
      "Epoch 2/5\n",
      "2410/2410 [==============================] - 6913s 3s/step - loss: 0.3930 - acc: 0.8499 - hamming: 0.1548 - val_loss: 0.3669 - val_acc: 0.8573 - val_hamming: 0.1484\n",
      "Epoch 3/5\n",
      "2410/2410 [==============================] - 6974s 3s/step - loss: 0.3644 - acc: 0.8585 - hamming: 0.1466 - val_loss: 0.3440 - val_acc: 0.8651 - val_hamming: 0.1423\n",
      "Epoch 4/5\n",
      "2410/2410 [==============================] - 6974s 3s/step - loss: 0.3458 - acc: 0.8655 - hamming: 0.1418 - val_loss: 0.3277 - val_acc: 0.8719 - val_hamming: 0.1378\n",
      "Epoch 5/5\n",
      "2410/2410 [==============================] - 7232s 3s/step - loss: 0.3309 - acc: 0.8714 - hamming: 0.1371 - val_loss: 0.3128 - val_acc: 0.8792 - val_hamming: 0.1323\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 5\n",
    "dataset_perc = 1\n",
    "training_history = model.fit_generator(\n",
    "                generator = generators[\"training\"],\n",
    "                steps_per_epoch = int((N[\"training\"]*dataset_perc) // batch_size),\n",
    "                validation_data = generators[\"validation\"],\n",
    "                validation_steps = int((N[\"validation\"]*dataset_perc) / batch_size),\n",
    "                epochs = epochs,\n",
    "                callbacks = [tb_logs]\n",
    "                #use_multiprocessing = use_multiprocessing,\n",
    "                #workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.evaluate_generator(\n",
    "    generators[\"test\"],\n",
    "    int((N[\"test\"]*dataset_perc) // batch_size)\n",
    "    #use_multiprocessing = use_multiprocessing,\n",
    "    #workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_dir, \"{}.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(save_dir, \"{}.h5\".format(model_name)))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators[\"test\"].on_epoch_end()\n",
    "batch0_test = generators[\"test\"].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(batch_size):\n",
    "    x, y = batch0_test[0][i], batch0_test[1][i]\n",
    "    pred_y = np.reshape(model.predict(x.reshape(1, 12000, 1)), 10)\n",
    "    print(pred_y.shape)\n",
    "    print(\"Actual:\\n\\t{},\\n\\t{}\\nPrediction:\\n\\t{},\\n\\t{}\\n\".format(y, ys_to_labels(y), [round(p_y, 3) for p_y in pred_y.tolist()], ys_to_labels([int(round(p_y)) for p_y in pred_y.tolist()])))\n",
    "    predictions.append({\"pred\": pred_y, \"actual\": y})\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
